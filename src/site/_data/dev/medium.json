{"url":"https://medium.com/feed/netlify","posts":[{"title":["Adding Netlify CMS and redirects to Hexo site, the missing pieces"],"link":["https://medium.com/netlify/adding-netlify-cms-and-redirects-to-hexo-site-the-missing-pieces-c69a8ec053d1?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/c69a8ec053d1","$":{"isPermaLink":"false"}}],"category":["redirect","netlify","static-site-generator","internationalization","hexo"],"dc:creator":["Luna Yu"],"pubDate":["Thu, 26 Jul 2018 00:45:09 GMT"],"atom:updated":["2018-08-09T00:42:14.167Z"],"content:encoded":["<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*LC1x7InkJ0izDq95fJEm-A.png\" /></figure><p>Recently, I learned how to build and deploy my first Hexo site following <a href=\"https://www.netlify.com/blog/2015/10/26/a-step-by-step-guide-hexo-on-netlify/\">A Step-by-Step Guide: Hexo on Netlify</a>.</p><p>In order to create a better editorial experience and support internationalization, I added Netlify’s CMS and redirect rules to my site. You can find <a href=\"https://templates.netlify.com/template/hexo-starter-theme-material/\">my project on JAMstack templates</a> if you want to follow along. Now, sit back, grab a coffee and let me tell you what I discovered.</p><h3>Russian doll your repos — .gitsubmodule</h3><p>By default, Hexo generates boilerplate with a default theme called “Landscape”. It’s functional, it looks okay, but to be honest, it’s not what I want. It’s tempting to create a theme from scratch, but to quickly put together boilerplate with Netlify CMS and redirect rules, and to avoid maintenance worries, I decided to go with an existing one.</p><p>I found a popular theme called <a href=\"https://github.com/viosey/hexo-theme-material\">Material</a>. I forked its Git repository and then added a <em>.gitsubmodules</em> file to the root folder of my site’s repo to include the theme as a sub-folder. If you’re not familiar with Git submodules, you can learn the basics by following along with this article: <a href=\"https://chrisjean.com/git-submodules-adding-using-removing-and-updating/\">Git Submodules: Adding, Using, Removing, Updating</a>. There are lots of resources available to help you learn more advanced tricks about Git submodules. Google around if you want to dive deep.</p><h3>Add Netlify CMS to the site</h3><p>Following the tutorial “<a href=\"https://www.netlifycms.org/docs/add-to-your-site/\">Add Netlify CMS to Your Site</a>”, I created an admin folder under my site’s source folder, which includes an <em>index.html</em> and a <em>config.yml</em> file.</p><p><strong>Add Netlify Identity Widget</strong></p><p>Netlify CMS works hand-in-hand with <a href=\"https://www.netlify.com/docs/identity/\">Netlify’s Identity service</a>. To add it to my site, I needed to include the following script in the <em>&lt;head&gt;</em> of my CMS index page at<em> /admin/index.html</em>, as well as in the <em>&lt;head&gt;</em> of my site’s main index page.</p><pre>&lt;script src=&quot;https://identity.netlify.com/v1/netlify-identity-widget.js&quot;&gt;&lt;/script&gt;</pre><p>Normally, front matter is where we store and access data of a post. Hexo allows us to introduce external data and reuse it in different places by storing data files under <em>source/_data</em> folder. It’s like a “mini database” that we store modularized data. Both YAML and JSON format are supported.</p><p>My goal was to rewrite the <em>&lt;head&gt;</em> of my site’s main index page with the external script tag.</p><p>I noticed that in the <em>head.ejs</em> file under <em>themes/layout/partial</em> folder, the following condition is included to allow the use of a custom <em>&lt;head&gt;</em>.</p><pre><em>&lt;!-- Custom Head --&gt;</em></pre><pre>&lt;% if (site.data.head) { %&gt;</pre><pre>  &lt;% for (var i in site.data.head) { %&gt;</pre><pre>    &lt;%- site.data.head[i] %&gt;</pre><pre>  &lt;% } %&gt;</pre><pre>&lt;% } %&gt;</pre><p>All that’s needed to do now is to store the Netlify Identity widget script inside a <em>head.json/yml</em> file and put it under the <em>source/_data</em> folder.</p><p>Tada! A Netlify Identity widget has been successfully added to the site. Now I can manage CMS admin users for my site without requiring them to have an account with my Git host or commit access on my repo.</p><p><strong>Skip render folder and files</strong></p><p>It turned out, there were some conflicts between the CSS of the theme and the Netlify CMS admin interface. I noticed that Hexo allows us to specify folders and files that shouldn’t be rendered by the Hexo engine by adding the path to <em>skip_render</em> rule in the root <em>_config.yml</em> file.</p><p>Here’s what I did. I added the following rule to tell Hexo not to render anything under the admin folder.</p><pre>skip_render:</pre><pre>  - &quot;*admin/*&quot;</pre><p>It worked! Now the admin interface is no longer overlapping the site home page.</p><p><em>This might not be necessary, depending on what kind of theme you use. It’s just something to tuck under your belt in case you bump into the same issue.</em></p><h3>Internationalization and redirects</h3><p>Sometimes site content needs to be available in different languages for members of the audience who do not read in English. We can certainly make a site more personalized by using Hexo’s internationalization/localization and Netlify’s redirect rules.</p><p><strong>Multi-language support</strong></p><p>First, make sure you have all the language files added to the language folder under the theme folder. Our example theme has it set up for us.</p><p>Secondly, change the language setting in the <em>_config.yml</em> file to specify the languages you want to present your site.</p><pre>language:</pre><pre>  - [language_01]</pre><pre>  - [language_02]</pre><pre>  - [language_03]</pre><p>By default, the <em>i18n_dir</em> is set to be <em>:lang</em>, which means that Hexo will detect the language within the first segment of URL. This plays well with our redirect rules that’ll soon be added.</p><p><strong>Add redirect rules</strong></p><p>Netlify’s <a href=\"https://www.netlify.com/docs/redirects/#geoip-and-language-based-redirects\">GeoIP and language based redirects</a> feature allows us to show targeted content according to geo location and browser preference. In our case, we’d like to show posts in our user’s preferred language.</p><p>You can add specific rules in a _redirects file under your root directory or <a href=\"https://www.netlify.com/docs/redirects/#structured-configuration\">add more structured rules</a> in a <em>netlify.toml</em> file. The following examples are created in a <em>_redirects</em> file.</p><p>Example code of location based redirect rules:</p><pre>/ /china 302 Country=cn,hk,tw</pre><p>Example code of language based redirect rules:</p><pre>/ /zh-cn 302 Language=zh</pre><p>Now, if your users reside inside China, Hong Kong or Taiwan, they’ll be taken to <a href=\"https://yourwebsite.com/china\">https://yourwebsite.com/china</a> when they type in the site URL. And if the language preference of their browser is set to be zh-cn, they’ll be taken to <a href=\"https://yourwebsite.com/zh-cn/\">https://yourwebsite.com/zh-cn/</a>. You can apply this technique for any other languages.</p><p>Feel free to do whatever you want with your folder structure. But make sure that the file paths in the redirect rules match your folder structure. E.g., I created a zh-cn folder for all the pages in Chinese and added an about folder with an about page:</p><pre>source/</pre><pre>├── _data</pre><pre>│   └── head.json</pre><pre>├── _posts</pre><pre>│   ├── Platero</pre><pre>│   │   └── platero.jpg</pre><pre>│   ├── Platero.md</pre><pre>│   ├── hello-world.md</pre><pre>│   └── 你好.md</pre><pre>├── _redirects</pre><pre>├── admin</pre><pre>│   ├── config.yml</pre><pre>│   └── index.html</pre><pre>├── images</pre><pre>│   └── uploads</pre><pre>│       └── netlify-logo.png</pre><pre>└── zh-cn</pre><pre>├── about</pre><pre>│   └── index.md</pre><pre>└── index.md</pre><p>You might have noticed that in my <em>_redirects</em> file, I specified <em>/zh-cn/</em>about as a redirect rule for the about page in simplified Chinese. So when somebody visits <a href=\"https://yourwebsite.com/about\">https://yourwebsite.com/about</a>, they’ll be redirected to <a href=\"https://yourwebsite.com/zh-cn/about\">https://yourwebsite.com/zh-cn/about</a> if their language preference is <em>zh-cn</em>.</p><p><strong>Single language home page and archives page</strong></p><p>Time to get a bit fancier. By default, our theme shows all the posts in all languages on the home page. That doesn’t look great when we apply our redirect rules and target content based on language preference.</p><p>Luckily, I found a plug-in called <a href=\"https://github.com/xcatliu/hexo-generator-index-i18n\">hexo-generator-index-i18n</a><strong> </strong>to solve the problem. It generates an index page for each language you have. And you can add configuration in your <em>_config.yml</em> file to show only one language on the index page.</p><p>Here’s how it works.</p><p>First you need to remove the existing plug-in in your <em>package.json</em> file:</p><pre>&quot;hexo-generator-index&quot;: &quot;^0.2.0&quot;</pre><p>Then, install the new plug-in. And then you’ll need to add <em>single_language_index</em> under <em>index_generator</em> in your <em>_config.yml</em> file, and set the value to be <em>true</em>.</p><pre>index_generator:</pre><pre>path: &#39;&#39;</pre><pre>per_page: 10</pre><pre>order_by: -date</pre><pre>single_language_index: true</pre><p>Also, add <em>:lang</em> in permalink so that the post links match the redirect rules.</p><pre>permalink: :lang/:year/:month/:day/:title/</pre><p>Now, if you go to the home page again, you should be only seeing posts in one language.</p><p>Same applies to the archives directory. You can use the <a href=\"https://www.npmjs.com/package/hexo-generator-multilingual-archive\">hexo-generator-multilingual-archive</a> plug-in to localize archived posts, except that you don’t need to modify the <em>_config.yml</em> file.</p><p>That’s it. I hope this is helpful to you. Feel free to connect on <a href=\"https://twitter.com/lunaceee\">twitter</a>. I’d love to hear from you!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c69a8ec053d1\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/adding-netlify-cms-and-redirects-to-hexo-site-the-missing-pieces-c69a8ec053d1\">Adding Netlify CMS and redirects to Hexo site, the missing pieces</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"adding-netlify-cms-and-redirects-to-hexo-site-the-missing-pieces-c69a8ec053d1"},{"title":["Migrating Netlify’s Continuous Deployment infra to Kubernetes (and everything we learned along the…"],"link":["https://medium.com/netlify/migrating-netlifys-continuous-deployment-infra-to-kubernetes-and-everything-we-learned-along-the-1e5989254269?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/1e5989254269","$":{"isPermaLink":"false"}}],"category":["devops","scalability","infrastructure","kubernetes","incident-management"],"dc:creator":["David Calavera"],"pubDate":["Wed, 11 Jul 2018 19:53:13 GMT"],"atom:updated":["2018-07-11T19:56:16.031Z"],"content:encoded":["<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*S0GX2pn5YTaCf3ZH\" /><figcaption>Photo by <a href=\"https://unsplash.com/@peppemessomalex?utm_source=medium&amp;utm_medium=referral\">Giuseppe Murabito</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>Migrating Netlify’s Continuous Deployment infra to Kubernetes (and everything we learned along the way)</h3><p>During 2017, Netlify suffered a series of incidents that made us reconsider the way a key component in our infrastructure works. This post is about the journey we took to make our service more resilient. I’ll also share some thoughts that hopefully will help you approach risk and safety management from new points of view.</p><h3>The need for certainty</h3><p>One common issue among the incidents I’ll describe is that we weren’t certain about what triggered each one of those events. Humans, in general, prefer simple explanations that point to specific causes because it means that a single change may be sufficient to correct the problem.</p><p>In Nietzshe’s words:</p><blockquote>Extracting something familiar from something unknown relieves, comforts and satisfies us, besides giving us a feeling of power.</blockquote><blockquote><em>— Friedrich Nietzsche, Twilight of the Idols.</em></blockquote><p>He was talking about his fellow philosophers here, however, I think we all can relate to this feeling when we talk about incident investigation and outages. We tend to stop when we find a reasonable explanation to the event with familiar terms. Having a good understanding of the issues make us also feel like we have the system under control again.</p><p>Unfortunately for us, we didn’t have simple and reasonable explanations for those events. Which led us to start thinking about ways to replace the whole system.</p><h3>Systems thinking</h3><p>For those of you not familiar with Netlify, it’s a platform for building, deploying, and managing JAMstack projects. We give frontend developers a modern workflow that combines globally-distributed deployments, continuous integration, and many other features for the modern web.</p><p>The system that concerns us here is the one in charge of building and deploying customer projects. Its only purpose is to take the source code that lives in a Git repository, run some build commands to generate all the files we need to deploy, and upload those files to our origin servers.</p><p>This diagram represents the architecture of the system that was in production between 2015 and January 2018.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*TC-yYLP0DchSyd7V.png\" /></figure><p>Our API receives events from different Git sources, like GitHub. It stores information about this event in a database, and sends a message to a broker queue. On the other end, a microservice deployed in a cluster of virtual machines listens for messages in the queue. Upon receiving a message, it starts a docker container that runs the build process necessary to generate a new deployment.</p><h3>Capacity Management</h3><p>The first challenge we faced with this architecture was capacity management. As our customer base increased exponentially, the demand for infrastructure resources increased in the same fashion. This new traffic in our build network caused degradation of service on several occasions, which made us consider the different strategies for capacity planning.</p><p>At the beginning, we started using a lead strategy. We increased the number of VMs with the goal of being over-provisioned, without paying attention to the traffic. That was a good solution to start with because we had several thousands of credits in different cloud providers and we didn’t have to be concerned about paying those bills.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Bev7cGD1s9ZoUqSm.png\" /></figure><p>The problem with this strategy is that we used arbitrary numbers with the hope that it’d be enough to keep up with demand. This was also a waste of resources on a daily basis, because the traffic that goes through our network on a Monday morning is completely different from that on a Saturday night.</p><p>The bad news is that being over-provisioned today doesn’t mean anything tomorrow, and every now and then, we woke up to alarms of traffic spikes and build queues backed up as a result. Our responses to these events were always reactive to match the spike of traffic, without having simple explanations for why they happened.</p><p>Increasing capacity in this way is known as lag strategy. It’s a more conservative strategy that doesn’t yield good results, since you usually wait until there are delays in your system to add more capacity, eroding confidence in the service as a whole.</p><p>What we really needed was a system that could adjust the capacity to match demand in each situation, and do it in an automatic way, without us having to worry about how expensive it was, or how many customers were experiencing delays in their deploys.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*0Wx6qClWRelBLJvG.png\" /></figure><h3>Observing Normal Accidents</h3><p>Delays in building and deploying projects was not the only problem we faced with this old system. Other issues landed in the area of what’s called <em>normal accidents</em>, or system accidents.</p><p>Charles Perrow presented this theory in his book <a href=\"http://www.worldcat.org/title/normal-accidents-living-with-high-risk-technologies/oclc/843204302\"><em>Normal Accidents: Living with High-Risk Technologies</em></a>, where he proposed that technical systems become so complex that unanticipated interactions of small failures are bound to lead to unwanted accidents and disasters.</p><p>In the distributed systems world, we can see this theory in action in the work of Kyle Kingsbury with the <a href=\"https://jepsen.io/\">Jepsen</a> project. The most clear examples of this type of accident are network partitions that trigger byzantine errors in distributed systems. Once again, we’re reminded that <a href=\"https://aphyr.com/posts/288-the-network-is-reliable\">networks are not reliable</a> — we cannot count on having reliable message-passing through networks; therefore, we should <a href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\">design our systems to be partition tolerant</a>.</p><p>During 2017, our build system suffered several network partition accidents. These errors manifested in different ways, from lost messages in the queue, to split brain events in which we had to completely recreate our message queue cluster to recover from them. The worst part was that we could not find simple explanations to these events, which only increased the uncertainty of the system. The queue broker we were using didn’t provide any useful metrics, and even when we configured the system to always emit debug logs, we could not get any information whatsoever about the events.</p><p>These incidents were the main issue that made us consider migrating our build system to a completely different architecture. We needed a system that gave us stronger guarantees in case of normal accidents, and allowed us to observe and reason about them without the pressure of a customer-impacting incident in progress.</p><h3>Circuit Breakers</h3><p>The most common technique to handle these types of incidents is to put <em>circuit breakers</em> in place.</p><blockquote>You wrap a protected function call in a circuit breaker, which monitors for failures. Once the failures reach a certain threshold, the circuit breaker trips.</blockquote><blockquote><em>— Martin Fowler, </em><a href=\"https://martinfowler.com/bliki/CircuitBreaker.html\"><em>CircuitBreaker</em></a></blockquote><p>A circuit breaker is an abstraction in your code that forces you to define how your code is going to behave in case of system errors. It includes heuristics to decide whether to run your intended code or to fall back to code that handles errors. For example, you can define that if an action fails 30% of the time, then you should stop trying to perform that action and instead do something else.</p><p>We added circuit breakers to decide what to do in case of network partitions — if a service cannot talk to another through the network, we need to decide how to handle that problem. In this particular scenario, we used circuit breakers to stop sending messages to the message broker if our API could not reach it. The code looked like this:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/06a2f5040afd14f48ea747313f8ba9b5/href\">https://medium.com/media/06a2f5040afd14f48ea747313f8ba9b5/href</a></iframe><p>During network partitions, we stored all messages in a secondary store until we recovered from the incident. At that point, we re-enqueued the messages in the order we received them. This gave us the confidence that we didn’t miss any messages during an outage, and we could recover from the incident by responding to those messages in a more controlled manner.</p><p>However, the big question that we still had to answer was how we could prevent all these incidents from happening in the first place.</p><h3>The rise of Kubernetes</h3><p>We evaluated different solutions to the problems we faced with our build system. For a more conservative approach, we could replace our queue system with a different one that had been proven more reliable. This could help us mitigate normal accidents, but it didn’t solve our capacity planning issues. We also considered writing a custom scheduler and load balancer that could adjust capacity based on user demand. This could solve our capacity issues, but of course, it wouldn’t help mitigate normal accidents. Plus, we’d have to spend time finding and developing the right solution, and it could also introduce new unknowns.</p><p>We needed a more pragmatic solution. Our team is not very big, so we had to consider the time we’d spend building and managing a new system. We re-assessed the problem as a whole, and looked for existing technologies to meet our needs. We had three requirements:</p><ul><li>- Proven stability against normal accidents.</li><li>- Ability to schedule jobs, maximizing CPU and memory efficiency.</li><li>- Automatic capacity adjustment, adding and removing resources on demand.</li></ul><p>This made us look into Kubernetes. Today, there is an enormous amount of information about Kubernetes out there, and the community is only growing. This gave us confidence that in the event of an accident, we could find good solutions. We also decided to use the managed service of one of our cloud providers, instead of managing Kubernetes ourselves. This meant that experts in the field could help us make it reliable while we focused on our own system problems.</p><p>After some experimentation, we were fairly certain that we could use the Kubernetes API to schedule pods to run our builds and deploys, and we could also leverage the Kubernetes Autoscaler to manage the cluster’s capacity automatically for us.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*FY-jBznBtQvvu9Yl.png\" /></figure><p>At Netlify, we’re not exactly fans of the idea of “build a huge new system, switch it on, and hope for the best. We had a migration plan in place to help us evaluate our progress and ensure that the new system could make our build service more resilient.</p><h3>The ETTO Principle</h3><p>Erik Hollnagel published a theory a few years ago on the trade-off of being efficient versus being thorough. In his book, he explained that it’s impossible to maximize efficiency and thoroughness at the same.</p><blockquote>The ETTO principle refers to the fact that people (and organizations) have to make a trade-off between the resources they spend on preparing an activity and the resources the spend on doing it.</blockquote><blockquote><em>— Erik Hollnagel, </em><a href=\"http://erikhollnagel.com/ideas/etto-principle/index.html\"><em>The ETTO Principle</em></a></blockquote><p>In other words, we needed to make a trade-off between the time and effort we spent designing and testing the new system versus the time customers could be spending on the new, presumably better, system. If we chose thoroughness and took longer to migrate to the new system, we could face new accidents in our old system, which would erode the customer confidence and risk our SLAs. If we chose efficiency, we could have migrated to the new system quickly, but face many unknown system accidents, which would have risked our SLAs too.</p><h3>Feature flag all the thing</h3><p>We use feature flags across our platform extensively. They allow us to expose customers to new features in a controlled manner. But what’s more important to us is that they also allow us to put features in production and expose them for our use only. I’m a big proponent of testing in production when done right, and feature flags are a fundamental part of it.</p><blockquote>Once new code is deployed to production behind a feature flag, it can then be tested for correctness and performance as and when required in the production environment.</blockquote><blockquote><em>— Cindy Sridharan, </em><a href=\"https://medium.com/@copyconstruct/testing-in-production-the-safe-way-18ca102d0ef1\"><em>Testing in Production the Safe Way</em></a></blockquote><p>A good feature flag system gives you the confidence that new parts of your code are only visible under certain conditions. It turns a feature on and off instantly on demand, and in a selective way. It should also tell you how many users a feature is enabled for and how often a customer reaches that feature.</p><p>At Netlify, we specify feature flags at three levels: global flags, account flags, and project flags. Those levels propagate on cascade — a flag enabled for an account applies to all projects within it, and a flag enabled globally applies to all projects in all accounts. We can also enable flags for a percentage of accounts and projects, as well as creating groups based on characteristics, like only accounts owned by teams.</p><p>The first step to migrate to our new build service was to define a feature flag that could route traffic to this service. At the beginning, the flag was only enabled for a few developers working on the service. When we started to get comfortable with the new service, we enabled it for the company web projects. The code for this feature flag looked like this:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/fb137dc3dfe1527c152adc144a297a11/href\">https://medium.com/media/fb137dc3dfe1527c152adc144a297a11/href</a></iframe><h3>Planning rollbacks and recoveries</h3><p>Once we had a feature flag in place, we needed to consider our options to rollback traffic from the new service to the old service in case of unexpected accidents.</p><p>We started by storing metadata about which service had been used for each build, so we could identify affected customers. With that information at hand, our API could replay builds in the old service if we had to roll back and recover due to issues with the new service. From the point of view of an affected customer, they’d see Netlify failing to deploy one of their builds and automatically starting it again. The code for this logic looked like this:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/900be00b432a36abd98166a8ad48ee33/href\">https://medium.com/media/900be00b432a36abd98166a8ad48ee33/href</a></iframe><h3>Challenges scaling the service</h3><p>At the same time we were building the service, we ran several load tests to ensure that Kubernetes’ autoscaler would work correctly, scaling up in the event of increasing build demands, and scaling down during low traffic times, like weekends.</p><p>This is the point were we bumped into our first complication. The autoscaler took too long to bring new nodes up when it needed to increase capacity. When Kubernetes adds additional capacity, the scheduler doesn’t send jobs to the new nodes until the docker images for those jobs are completely downloaded in the node. In our case, there is only one image to download, but that image’s size was 4 gigabytes, which took about 9 minutes to download. This was unacceptable as a capacity management measure, since many customers would be waiting for at least 9 minutes to get their builds started.</p><p><strong>Specialized vs. general-purpose Docker images</strong></p><p>If you’re familiar with Docker, you probably know that building images specialized for a specific purpose is usually a good practice. For example, you have an image for Nginx and its only purpose is to run Nginx. You also have an image for Go, and its only purpose is to run Go commands, and so on. In traditional Continuous Integration services, this is very convenient — if you want to run tests for your Rails application, you use an image with Ruby and Rails installed, and nothing else. This also helps keeping the image sizes small, which is more efficient when you have to download them many times from a container registry.</p><p>The nature of the projects our customers build on Netlify actually makes this less convenient. Usually, a web project will involve some JavaScript compilation, so we need Node. It might also need another programming language if you use a site generator, or libraries to process image files. Having a specialized Docker image would actually be a disadvantage for us because our customers would have to modify them for each one of their use cases.</p><p>However, we could improve how we installed packages in our image. At one point, we had five different versions of Ruby installed by default, and several versions of Python. To reduce size, we removed everything that didn’t need to be installed as root, and implemented several ways to lazy install those packages. For major the programming languages that we support, we use specific package managers like NVM and RVM to install Node and Ruby versions on demand. After the first installation, this version is stored in a local cache, so the next time this customer needs it, it doesn’t need to be installed — we fetch all the libraries from the cache, already compiled for the right system.</p><p>This work reduced the size of the image by about 2 gigabytes, but it still took several minutes to install the image. The problem was that even though Docker downloads layers in parallel, the process to build the image from those layers is sequential. As of this writing, our image includes 74 layers, and uncompressing those layers took much longer than downloading them. Fortunately, Docker includes a new squash utility that collapses all the layers in your image into a single layer. When we make changes to the image, we use Jenkins to squash the layers and push it to our private registry. Using a squashed image reduced the time it took for a new node to be ready to accept jobs from minutes to seconds.</p><p><strong>Disk saturation</strong></p><p>After some favorable results with the autoscaler, we decided to turn the feature flag on for a small percentage of our customers. This worked pretty well, and we started to see very promising results. Customer builds and deploys were going through the new service, and we even noticed a small drop in deploy times. So we enabled it for a higher percentage of customers.</p><p>At this point we started to see some very weird behavior. Every now and then, Kubernetes killed the Docker daemon along with all the containers running in that node, causing builds to be aborted in the middle of the process. We were very puzzled by this behavior.</p><p>We traced that behavior to a script for monitoring container runtime in GCE:</p><p><a href=\"https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/health-monitor.sh#L27\">https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/health-monitor.sh#L27</a></p><p>This health-monitor script uses the docker ps command to decide whether a node is healthy or not. If that command takes longer than 60 seconds to reply, Kubernetes considers the node unhealthy and restarts the Docker daemon. However, there was a known problem where docker ps would hang indefinitely for reasons that were never very well understood. If you google “Docker ps hangs”, you’ll get about 30,000 results back.</p><p>Docker solved the problem in version 1.13, but because our service provider managed our cluster, updating the Docker daemon was not an option. Instead, we would have to figure out why our docker ps commands were hanging, and prevent it. In our case, the cause was that the node’s disk was saturated due to jobs manipulating too many files.</p><p>We found an alternative solution by mounting two different disks in each node. We use one disk for the Docker filesystem, where Docker stores information about the containers running. We use the other disk for our own workloads, where customers run their build commands.</p><p>Once we moved to a double disk configuration, we stopped observing disk saturation and daemon restarting issues. With this change in place, we turned the new service on for all our customers. Weeks later, we decommissioned the old service and removed the code that scheduled builds in it from our production API.</p><h3>Afterthoughts</h3><p>At the beginning of this project, we estimated that it would take us two months to migrate to Kubernetes. In the end, it took us about four months to finally put it in full production. The main two causes for this delay were:</p><ul><li>- Bumping into unknown accidents that we had not planned for.</li><li>- Not having direct access to the Kubernetes cluster management.</li></ul><p>On the other hand, both of these issues had benefits as well. Some of the accidents we bumped into forced us to improve parts of our infrastructure that we had neglected for some time. Delegating cluster management to a service provider shielded us from other types of accidents that we didn’t have to worry about.</p><p>In the six months since we migrated our build service to Kubernetes, we’ve suffered zero system accidents in the service. The times where the cluster was under-provisioned, the autoscaler worked as expected and automatically increased capacity without customers noticing any impact. An interesting byproduct of this is that we can now observe and reason about these autoscaling events without the time pressure and communication requirements of a customer-impacting incident.</p><p>Also in the last six months, the number of builds we process has grown by 50%. However, the direct cost of our build service has remained stable — we’re spending only about $200 per month more than we were before. If we had kept the old service, our expenses would have increased at the same rate the traffic increased.</p><p>This is not the end of the journey for our build service, and we’re now in a better place to take it to the next level. If you’re interested in challenges like this, <a href=\"https://www.netlify.com/careers/\">we’re hiring</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1e5989254269\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/migrating-netlifys-continuous-deployment-infra-to-kubernetes-and-everything-we-learned-along-the-1e5989254269\">Migrating Netlify’s Continuous Deployment infra to Kubernetes (and everything we learned along the…</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"migrating-netlifys-continuous-deployment-infra-to-kubernetes-and-everything-we-learned-along-the-1e5989254269"},{"title":["Off with their heads — the rise of the modern CMS"],"link":["https://medium.com/netlify/off-with-their-heads-the-rise-of-the-modern-cms-e0089538aed6?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/e0089538aed6","$":{"isPermaLink":"false"}}],"category":["cms","web-development","development"],"dc:creator":["Phil Hawksworth"],"pubDate":["Thu, 14 Jun 2018 00:04:44 GMT"],"atom:updated":["2018-06-14T14:26:58.924Z"],"content:encoded":["<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*A8GBoJAj_Rv-ZsX9DcticA.jpeg\" /></figure><p>Wasn’t this supposed to be easy?</p><p>You paid a glamorous agency to design your new website. You spent thousands of hours and a gazillion dollars laboring over every aspect of the design to get it just right. After rounds and rounds of meetings and reviews and arguments and sweat and tweaks and nudges… finally you have a design you are happy with. Even the board is happy with it. And they want it. They want it now.</p><p>So let’s build this sucker. And let’s launch it. Then you can finally say thank you and goodbye to those expensive design and development contractors and get on with managing the content in your gleaming new website yourself. For you, in your wisdom, chose to invest in <em>the best</em> CMS! And now the web is your oyster.</p><p>No?</p><p>No. Usually not.</p><h3>An expensive legacy</h3><p>Content Management Systems, or CMS, often require incredible levels of financial, technical, and even political investment. For years they have promised clients the opportunity to take control of their websites without the need to write code or understand infrastructure. But more often than not, they leave their aspirations of being liberated and creative in tatters.</p><p>Back in the late and early nineties, CMS were expensive. Very expensive. No, even more than the number you are probably thinking of now.</p><p>The industry was dominated by exotic software solutions, sold by serious sales executives with silver business card holders. Comfort in open source software or a product that wasn’t delivered in sturdy, cling-wrapped presentation cases was yet to take hold.</p><p>I undertook my first project to select a CMS vendor back in 2001 when there were far fewer options available. I worked for a software company who had built and were hosting a website for an insurance firm. They wanted to be able to update their news page, and perhaps one day, edit the phone number in their footer without getting any of that messy HTML stuff all over their fingers.</p><p>They asked us for a CMS, so we went shopping.</p><p>The first two products we found were from companies with offices in London. Quite locally to us as luck would have it, because in order to purchase a license for the CMS from either of these companies we would first need to sit in their office across a large board room table. They were keen to talk about how we would have to pay them just under £1M for the licenses. (I’m unsure of the exchange rate back in 2001, but can we just agree that whatever it was, this is, in layman’s terms, a crap ton of money?)</p><p>I wore my best suit to that meeting. I didn’t want to look like a fool while we negotiated a £1M deal to make it easier for my client to update the contact details in their footer.</p><p>Of course, the use case for a CMS usually goes further than this. But back then, <em>any</em> flexibility came at an alarming price.</p><p>Paying this sort of money for the license (not the hosting infrastructure, not the consultancy, not the training for the developers and authors, not the design nor the build, <strong>just the license</strong>) surely demonstrates impressive levels of eagerness to remove developers from the equation. Or, perhaps, an over-optimistic view of just how much ease and freedom this kind of tool will provide.</p><p>Luckily though, times have changed. Yes, you can still pay a lot of money for a CMS, and the challenges in using them effectively are well documented. But over the years more people put their weight behind trying to solve this problem. New players have entered the market making the cost and the scarcity of the skills come down. The days of giant CMS vendors dominating the entire market may be numbered. And now we seem to be on the verge of something of a revolution. A CMS is now attainable on any size of budget and without a team of specialized consultants to roll it out.</p><p>And I no longer need my suit for when I’m searching for the right CMS.</p><h3>Why was this so difficult?</h3><p>Before we look at the new approach to CMS which is rising in popularity, it’s worth understanding the some of the history, and some of the challenges which have led legacy models to be challenged.</p><p>It once seemed that we had just a small number of complex CMS products available to us. Our choices were extremely limited and things were expensive as a result. But time brought greater acceptance of open source software, and with it, more and more attempts to deliver affordable and approachable CMS solutions.</p><p>For example, Drupal grew from a humble message board platform in 2001 to an open source CMS supported by a large community of active developers. Around the same time Wordpress started to bring CMS-like features to a growing community of bloggers. Both projects were empowered by the availability and relatively low cost of infrastructure available for hosting PHP and mySQL.</p><p>Other projects began to emerge as more people tackled the challenge of competing with the larger, established CMS vendors. As an industry, we were discovering that we could meet some of the technical challenges inherent in a CMS, and that was empowering. But our approach to usability and also safeguarding front-end code left quite a bit to be desired.</p><p>The market started filling up with products which were trying to compete on the basis of how many features they had. The more features and flexibility a product could list, the more desirable, future proof, and valuable it was deemed to be. This was a dangerous path.</p><p>We came to expect a CMS to do far more than manage content. It’s a huge misnomer. The most popular and expensive CMS often have a laundry list of tempting features. We expect them to allow us to do everything from customizing our page layouts, to powering e-commerce, to handling user-generated content — all while generating front-end code and being a core part of the hosting infrastructure serving a site to a global audience.</p><p>Bundling all of these capabilities into one, monolithic piece of software is incredibly challenging. Each area is full of craft and nuance and subtlety, Yet vendors have tried to package them up with an admin interface for anyone to manage through the click of some buttons.</p><p>Managing your site CMS became insufferably difficult. You needed experts to help you use it. And what it delivered fell short of the standard people wanted.</p><p>When we try to design a product capable of doing everything for everyone, we often find ourselves with a product so complex that nobody can use it for anything.</p><h3>Focus</h3><p>It sounds like I’m stating the obvious, but so many of the features done poorly by a legacy CMS relate to managing the <em>presentation</em>, rather than just the content. After your huge investment to establish the design of your site (remember how excited the board were?), you run the risk of undermining the design with every single content edit.</p><p>A good CMS should protect your design investment. Not help you to destroy it.</p><p>Happily, the<a href=\"https://headlesscms.org/\"> Headless CMS</a> approach has been gaining momentum, and it’s all about focussing on doing one thing well. That thing is managing content<em>.</em></p><h3>How do headless CMS work?</h3><p>Headless, or decoupled CMS, provide the ability to define the structure of your content, offer an interface to populate, manage the content in that defined structure, and then serve the content via content APIs.</p><p>In this way, the management of content is not wrapped up with the ability to modify the design. The presentation of the content through the carefully crafted interface, is handled away from your CMS, protecting it from that overzealous content author, who thinks that the headings in <em>their</em> article really need “a little extra pop”.</p><p>The admin interface a headless CMS provides is not a part of the infrastructure you host to serve your site. Putting distance between the mechanics of managing your content (along with various user management and publishing workflow) and the mechanics of hosting your site is extremely attractive.</p><p>When the CMS was part of the hosting infrastructure it would typically compile the page a visitor requested at the time they requested it. That involved activities like asking a database what content should go into which template and cooking it up to serve <a href=\"https://www.merriam-webster.com/dictionary/%C3%A0%20la%20minute\"><em>á la minute</em></a>. This means that both your site and your CMS would need to be able to scale to handle the load of whatever you throw at them.</p><p>Having a level of abstraction bodes well for scaling and security. The more distance we can place between traffic to our site, and the moving parts which drive the CMS, the better our chances of protecting it from those with malicious intent.</p><p>Added peace of mind.</p><h3>Performance and craft</h3><p>The benefits of decoupling the management of the content from the control of the design go beyond the aesthetic we discussed earlier. They impact performance, too.</p><p>When a traditional CMS allowed authors to manipulate the design of your site, it needed to generate some of the code for the site automatically. Features like drag and drop and<a href=\"http://alistapart.com/column/wysiwtf\"> WYSIWYG</a> editors bring with them code generated automatically for you by the system.</p><p>Most front-end developers will start fidgeting at that thought. And I’m right there with them.</p><p>This generated code was devised long before your site was ever being discussed. It was not made for you. It was made to serve a generic purpose. It has been designed to be massively flexible so that it can be used time and time again. That’s hard to do and so we often pay a penalty for it as it introduces a variety of<a href=\"https://en.wikipedia.org/wiki/Code_smell\"> code smells</a> into our sites. I’ve grumbled about this before. You never want visitors to your site to be able to<a href=\"https://vimeo.com/53317254\"> smell your CMS</a>.</p><p>Developers responsible for the performance and browser support of a site need control over its code if they are to do a good job of delivering on the promise of the design. A headless CMS gives them back this control by being agnostic to the tools which consume it. In this age of responsive web design and broadening contexts for where and when our visitor use our sites, keeping control over how the code is crafted, in the hands of the experts could not be more important.</p><p>Trends in web development continue to advance. As browsers and devices evolve, we need the ability to employ the best techniques possible when rendering the site into its various templates. Abstracting the content via a headless CMS creates a clean separation which allows us to render it with whatever build tools, <a href=\"https://en.wikipedia.org/wiki/Comparison_of_web_template_engines\">templating language</a>, or<a href=\"https://www.staticgen.com/\"> static site generator</a> we might choose.</p><h3>Content portability</h3><p>With a headless CMS, you can break out of the monolithic model where all of your eggs are in one basket and your content can reach only as far as your CMS could support. Not only can you select what tools generate and present the content on your site, you can also publish your content in new formats and into other channels.</p><p>For example, if a new RSS-like format was to be defined, or presenting content in AMP format were to become attractive, that would be no problem for a Headless CMS. The content is not coupled to any one presentation format. You can expose it with new templates to support emerging formats as they come along.</p><p>Another example, structured content served through APIs can more readily be consumed by email campaign tools, or social media campaign tools. It allows you to lean on the expertise of specialists in each of these fields. And on those of areas that we have not even considered yet.</p><p>Our content is liberated to go anywhere.</p><h3>Momentum and adoption</h3><p>There is growing enthusiasm for this approach. Not only from the developers who build on top of such architectures, or from content authors who have become more empowered and productive than before, but also from businesses looking to avoid the kind of investment and lock-in I was subject to. (I’m still not sure that we ever managed to update the phone number in that footer!)</p><p>The market for headless CMS products appears to be thriving.</p><p><a href=\"https://www.contentful.com/\">Contentful</a>,<a href=\"https://prismic.io/\"> Prismic</a> and<a href=\"https://www.siteleaf.com/\"> Siteleaf</a> are just a few of the players in a<a href=\"http://headlesscms.org\"> rapidly-growing space</a> that’s receiving lots of validation. (Contentful secured a<a href=\"https://www.contentful.com/blog/2017/12/04/contentful-series-c/\"> $28M series C funding round</a> at the end of 2017) These companies already have impressive client lists adding weight to the argument that this approach is suitable for big brands with high-traffic and richly-featured sites.</p><p>It seems that the positive results of using this type of CMS is becoming increasingly apparent to CMS customers, and even products such as Wordpress are evolving to also support a headless mode.</p><h3>Where next?</h3><p>Momentum towards a headless CMS model is helping to demystify and democratize what was once an exclusive and stuffy market. When they are not seen as the domain of only the big enterprise-grade vendors, all kinds of innovations spring forth.</p><p>The shift is not limited to the headless model alone.</p><p>We’ve seen CMS products which pursue simplicity by building atop<a href=\"https://getkirby.com/\"> file-based systems</a> instead of databases. We’ve seen<a href=\"https://graphcms.com/\"> CMS implementing GraphQL</a> to allow even more expressive and simplified querying of our content. We’re even starting to see CMS like <a href=\"https://www.netlifycms.org\">Netlify CMS</a> which solves common version control and integration challenges by delivering a natural authoring experience<a href=\"https://www.netlifycms.org\"> on top of Git</a>.</p><p>Whatever happens next, we should not expect that the only solutions to managing content on a site have to be overwhelmingly complex or prohibitively expensive.</p><p>Labelling something as “reassuringly expensive” needs to be a habit that we put behind us as we explore modern approaches to meeting old challenges and assess each one on its merits and not just on its price tag.</p><p>I reserve my suit mostly for weddings now. Although it’s getting a little snug around the waist.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e0089538aed6\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/off-with-their-heads-the-rise-of-the-modern-cms-e0089538aed6\">Off with their heads — the rise of the modern CMS</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"off-with-their-heads-the-rise-of-the-modern-cms-e0089538aed6"},{"title":["Leveling up: why developers need to be able to identify technologies with staying power (and how to…"],"link":["https://medium.com/netlify/leveling-up-why-developers-need-to-be-able-to-identify-technologies-with-staying-power-and-how-to-9aa74878fc08?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/9aa74878fc08","$":{"isPermaLink":"false"}}],"category":["development","programming","jamstack","javascript","learning"],"dc:creator":["Mathias Biilmann"],"pubDate":["Wed, 30 May 2018 23:11:25 GMT"],"atom:updated":["2018-05-30T23:23:13.442Z"],"content:encoded":["<h3>Leveling up: why developers need to be able to identify technologies with staying power (and how to do it)</h3><p>JavaScript fatigue has become a common phrase in the world of today’s front-end developers. It can seem like there’s a new hyped framework, architecture, command line tool, or SaaS developer service every day. The constant churn of new things can end up leaving developers more jaded than excited.</p><p>To avoid this, it’s important to build up a solid instinct for separating the technologies and products worth spending time on from the ones that will fade into obscurity after their 15 minutes of fame is over, their featured article on TechCrunch has faded to the archives, or the last passive aggressive comment on their “Show HN” is long forgotten.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/480/0*s04iIsDK4LHtvGIN.jpg\" /></figure><p>My journey as a programmer started almost 30 years ago when I got my first computer: a used Commodore 64 that would greet me with a blinking cursor as an entry into “Basic V2”.</p><p>Since then, the only constant in the world of development has been change, and the need to always be learning and discovering. Here are some thoughts on how I’ve been able to keep up along the way without drowning in the constant flow of novelties.</p><p><strong>Learn your history<br></strong>This might be a surprising bit of advice in an article about getting ahead of the pace of change, but to understand and evaluate contemporary technologies, you have to learn about the history of your field.</p><p>In a space that changes this much and this often, its easy to take for granted that the stream of releases are truly new. But technology tends to be surprisingly cyclical; what might seem new on the surface, tends to have deep historical roots below.</p><p>When Ruby on Rails came out in 2004 it had an almost-meteoric rise and an immense influence on the industry. At the same time, most of the ideas underlying the Model View Controller (MVC) pattern it was based on, as well as the foundational object orientation patterns from Ruby, went all the way back to the Small Talk programming environment from the late 70’s.</p><p>For developers who were fluent with the major web platforms at the time (PHP, Java, ASP), Ruby on Rails introduced not just a whole new language with a new syntax, but new concepts and a major new paradigm for meta programming. However, for developers that had followed the rise (and fall) of SmallTalk and the languages and platforms inspired by it, Ruby on Rails was full of familiar concepts (with a bit of new syntax and some adaptation from the world of Small Talk applications crafted unto the web). All they needed to learn was the (important, but not huge) differences between Ruby and Small Talk, and the conceptual differences between MVC for the web and MVC for a Small Talk application.</p><p>In a similar way, when React came out it seemed to instantly sweep aside a whole generation of JavaScript frameworks. Most of these had tried to transfer a Rails-inspired MVC model to the browser. To many developers it seemed to be a drastic departure from both the single page app frameworks relying on templates with two-way data bindings, and from the simpler libraries like jQuery. But at its core, React was inspired by ideas from functional programming languages (especially OCAML) that went all the way back to the early days of computing.</p><p>The creator of React, Jordan Walke, recently described how his own journey back in history gave him the background needed to build out React:</p><ul><li><em>For the longest time I just assumed “welp, I guess I’m just a weird programmer”. Then I finally took a course on programming language fundamentals (which used ML (SML) for much of the coursework) and I finally had some basic terminology to be able describe how I wanted to build applications. I also learned that the style of programming that I gravitated towards was neither weird, nor new and actually closer to some of the oldest philosophies of programming languages — philosophies which hadn’t ever become mainstream — philosophies that the industry had spent upwards of twenty years undermining (in my opinion, to their disadvantage).</em></li><li><a href=\"https://www.reactiflux.com/transcripts/jordan-walke/\"><em>https://www.reactiflux.com/transcripts/jordan-walke/</em></a></li></ul><p>For many front-end developers the journey into the more fully-fledged world of full-on state management in React with some form of “Flux” architecture like Redux, maybe combined with Immutable.js, can feel overwhelming. But for developers with a solid historical foundation who had been following the re-emergence of functional programming — and the concepts around it going back to the creation of LISP in 1958 — React reflects familiar concepts and ideas.</p><p>Even when actively trying to learn a new technology, history can be a helpful teacher. When Rails was first released, it was tough to come by material about it aside from a few online docs, tutorials, and the source code itself (more about source code later). However, a lot was written about the evolution of MVC through Small Talk to Objective C, and lots of lessons learned from working with meta programming and OOP based on message passing in the Small Talk world.</p><p>This can be a great tool for learning new technologies much faster: instead of reading the latest tutorials and the emerging documentation, figure out what they’re inspired by, what previous knowledge they draw on and build upon. Most likely the material about those older technologies, ideas and methodologies will be much more mature and you’ll find lots of lessons learned that most likely apply to the new take on the field.</p><p>A solid historical awareness gives you a really good toolset to ask the question: what is different this time? The answer (or lack of one!) to that question will very often determine the success or failure of a new technology.</p><p><strong>People, culture, and community matter<br></strong>It’s easy to think that tools and technologies are simply evolving on their own. For example, Object Oriented Programming became Functional Programming, text editors developed into full fledged IDEs, and dynamic languages transitioned into statically typed languages. However, new technologies and frameworks don’t just follow an evolutionary path on their own. They’re invented, built, and disseminated by humans, organizations, and communities.</p><p>When a new tool or technology emerges, it’s important to question both the technical underpinnings (How is it different? What underlying patterns does it build on?) and motivation (Why did someone choose to build this now? Who are the people that feel passionate about this? What problems does this technology solve for organizations?).</p><p>One of my favorite essays on why some tools win while others fade away is Richard P. Gabriel’s “<a href=\"http://dreamsongs.com/RiseOfWorseIsBetter.html\">The Rise of Worse is Better</a>” from 1989. It describes a possible reason why Unix and C overtook the LISP-based technologies — a reason that had nothing to do with the inherent qualities of the two solutions.</p><p>In the essay Gabriel describes a “worse-is-better” approach, the New Jersey school of design in contrast to the MIT/Stanford school, that weighs the simplicity of the implementation higher than the simplicity or correctness of the end-user interface. This focus allowed C and Unix to beat LISP in the market. C compilers were easier to implement, port and optimize than LISP compilers and this made it much faster for the Unix implementers to get software into the hands of the users. This lead to faster adoption and eventually meant that far more people (and companies) were invested in growing and improving the C/Unix ecosystem.</p><p>When viewing new technologies, understand not just what they aim to do, and how they are technically implemented, but also how they are going to spread and how they will grow a community. Often the technologies that become important to the mainstream programming community are those that have the best answers to those later questions, even in cases where they can seem like a step back on pure technology grounds.</p><p>But here’s the real trick: sometimes tools that are technologically way ahead of the curve are doomed to never get widespread adoption (I’m willing to bet a lot of marbles that we’ll not all be writing web-apps in the Idris language anytime soon). LISP never became mainstream, but so many of todays mainstream frameworks, languages, libraries and techniques owe a huge debt to the ideas it invented and explored, and even today learning LISP can bring lots of insight into future technologies.</p><p>If you can spot the tools that live in this intersection, then learning those might bring you your next developer super-power.</p><p><strong>Always</strong> <strong>Understand the “Why”<br></strong>Back when I started developing, the closest thing to StackOverflow was computer magazines with source code you could manually type into your terminal to get the programs running.</p><p>I’m a sloppy typer, and I could never manage to type in a complete program without errors along the way. This is actually one of the (admittedly very few!) advantages of computer program printouts versus copy and paste-able Stack Overflow snippets: to get it to work, you need to actually understand the code.</p><p>As developers we’re always working with looming deadlines and with a pressure to get new functionality, features, and bug fixes out in the hands of our users as fast as possible. I’ve seen developers that get so focused on getting something out there, they throw libraries and code snippets together without taking the time to understanding why it works. Or, they’ll see that something is broken and simply try different potential solutions without first taking the time to understand why the system broke in the first place.</p><p>Don’t be that developer. Make it a rule for yourself to never use a solution from Stack Overflow or elsewhere before you take the time to understand why that solution could work. Challenge yourself to go one step further and figure out what it would have taken for you to come up with that solution yourself.</p><p>Sometimes you’ll find an issue where a small change (maybe changing one library for another, calling a variation of the function you were using, etc.) solves a bug, but you don’t actually know why. Don’t settle at this point. Dig in and build up a mental model that lets you understand exactly why one solution failed and another worked. Very often this will lead to deeper learnings and you’ll discover patterns that might reveal undetected bugs lurking in other parts of your system.</p><p>Also approach new technologies in this way. Don’t focus on learning on the surface. Learning the syntax for a few different frameworks or languages won’t teach you much, but learning the decision making process below the surface of those technologies will fundamentally make you a better developer.</p><p>When all is said and done, the most important thing is not <strong>what you learn</strong> (which framework, which tool, which language), but <strong>what you learn from it</strong>.</p><h3>Putting these lessons to work</h3><p>Choosing the right tools isn’t always easy or obvious — even for the most prolific of programmers. There’s a constant trade-off between sticking to well known, trusted and reliable tools with few surprises, and adopting brand new technologies that can help solve problems in new and better ways. But, a little up front work can make successfully choosing and implementing new tools part of your development practice. Indeed, it is a practice, one that’s always evolving. Here are a few ways to apply the suggestions from this post.</p><p><strong>Learn your history<br></strong>Historical awareness provides a solid toolset to ask, “What is different this time?” The answer (or lack of one) often determines the success or failure of a new technology. New stuff is cool. New stuff is fun. But if you feel overwhelmed at the speed of it all and the occasional burst of JavaScript fatigue is kicking in, then slow down and remember that it’s a long game and that following the large trends is more important than constantly rushing to rewrite all your apps in the newest framework. Peter Norvig puts it great in his essay <a href=\"http://norvig.com/21-days.html\">“Teach Yourself Programming in Ten Years”</a>.</p><p><strong>People, culture and community matter<br></strong>Thanks to the meteoric rise of GitHub, Stack Overflow and NPM it’s a lot easier to get early insight into how a community will scale and how developers are responding to its ambitions. While contributors and stars can tell you a lot about projects that are already successful, they aren’t always great early indicators of success. However, you can use the same logic to help determine whether a project is likely be embraced by the community as you might already use to build your own software or choose which company you want to work for:</p><ul><li>Is there a clearly-defined vision?</li><li>Is there a clear user need?</li><li>Are the right people, resources, and documentation in place for this to scale?</li><li>Is it extensible? I.e., can it scale or adapt to serve emerging technologies or user types?</li><li>And perhaps, who is behind it?</li></ul><p><strong>Always understand the “why” behind a technology<br></strong>Don’t focus on the surface, but on the currents underneath. Learning the syntax for a few different frameworks or languages will get you by, but learning the decision-making process of those technologies will fundamentally make you a better developer.</p><p>Michael Feathers has a great list of <a href=\"https://michaelfeathers.silvrback.com/10-papers-every-developer-should-read-at-least-twice\">“10 Papers Every Developer Should Read”</a>. All are about foundational ideas on languages, architectures and culture and set a great baseline for understanding the ideas beneath so many of the trends that are still making waves in programming today.</p><p>Go forth and dive into all the new things! But do it at a pace that makes sense. A pace that gives you time to build the right kind of foundation. This eventually lets you adopt new technologies faster, understand them more deeply, and evaluate their staying power more thoroughly.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9aa74878fc08\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/leveling-up-why-developers-need-to-be-able-to-identify-technologies-with-staying-power-and-how-to-9aa74878fc08\">Leveling up: why developers need to be able to identify technologies with staying power (and how to…</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"leveling-up-why-developers-need-to-be-able-to-identify-technologies-with-staying-power-and-how-to-9aa74878fc08"},{"title":["How Netlify migrated to a fully multi-cloud infrastructure"],"link":["https://medium.com/netlify/how-netlify-migrated-to-a-fully-multi-cloud-infrastructure-cfca95bbbb0f?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/cfca95bbbb0f","$":{"isPermaLink":"false"}}],"category":["aws","distributed-systems","google-cloud-platform","rackspace","cloud-computing"],"dc:creator":["Ryan Neal"],"pubDate":["Mon, 14 May 2018 19:11:44 GMT"],"atom:updated":["2018-05-14T19:50:32.521Z"],"content:encoded":["<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Ny1P-Sbg22sx0PoSwqh-2w.png\" /></figure><p>Netlify’s platform team is responsible for building and scaling the systems designed to keep Netlify, and the hundreds of thousands of sites that use it, up and running. We take that responsibility seriously, so we’re constantly looking for potential points of failure in our system and working to remove them. Most of our platform is cloud agnostic, and since we favor an approach that minimizes risk, we wanted to extend that to include our origin services. This led to a project that culminated in us migrating our origin services between cloud providers on a recent Sunday night — without any service interruptions.</p><p>When you deploy a website to Netlify, your content automatically gets pushed to the edge nodes in our Content Delivery Network (CDN). If you make changes in your Git repository, your content is continuously and automatically synced with our origin servers and pushed to the CDN.</p><p>Our CDN runs on six different cloud providers, but up until recently, our origin servers relied on only one. That meant our origin servers were subject to the performance and uptime characteristics of a single provider. We wanted to make sure that in the face of a major outage from an underlying provider, anywhere in our network, Netlify’s service would continue with minimal interruption. Our origin services can now swap between three providers: Google Cloud (GCP), AWS, and Rackspace Cloud without downtime. This post will cover how we planed, tested, and executed our first multi-cloud migration</p><p><strong>Planning for change</strong></p><p>The main goal for this project was to have a multi-cloud production system where we could direct traffic to different providers on demand, without interruptions. Our primary sources of data are the database and the cloud store — the first for keeping references to the data and the second for the data itself. To keep latency down, we needed to run a production-ready copy of both sources in every provider. They also needed to have automatic fallbacks configured to the other clouds. For example, Google instances prioritize Google Cloud Storage (GCS) and fallback to S3 automatically.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*Hua_U5CpUMKb-hzbK_DCxQ.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*2U5adrGH6LEYxQkREiIoYg.png\" /></figure><p>We first built a tool called <a href=\"https://github.com/rybit/cloud-bench\">cloud bench</a> that would check the performance of uploads to different cloud storages. It showed us what we suspected: upload to a cloud storage was faster if it stayed in the same provider. We also confirmed that pricing was better if uploads stayed inside the same network.</p><p>We went through the different services that touched the data in the cloud provider, extracting simple Get/Put interfaces for that data and implementing concrete versions for Google, Amazon, and Rackspace. We then released the service, with the abstracted interface, that would use Rackspace — our current cloud provider. Then we started to test that service with the different implementations for the other clouds.</p><p>AWS and GCP offer more robust load balancers than what we were using. Rackspace is limited to around 10GB/s, which is a lot under normal operation, but not in the event of DDoS. Both Google and Amazon don’t place any limits, saying that the frontend load balancers will scale up as needed to support the traffic.</p><p><strong>Data Replication</strong></p><p>Netlify customers have deployed terabytes of data and they’re deploying more every day. Before the migration, all of this data was stored in a single service: Rackspace’s Cloudfiles. Our first challenge was to build a system able to replicate the millions of blob objects we stored and start replicating new data as it came in.</p><p>Our approach attached a bitmask to the object reference that indicated where it was replicated. We did this to keep down the bloat on the database as there are millions of entries and adding long strings to each would balloon the database size. It also meant that any service that needed to use that blob now knew from where it could load the data. This bitmask opened the way for intelligent prioritization in different services.</p><p>We built a Go service that would inspect an existing blob, figure out where it needed to push it to, actually push it, and record its progress. The service was able to act in both online replication and batch backfilling modes.</p><p>We fed a few million objects to the service for replication. After manually checking those blobs, we started two processes. One that would process newly-uploaded objects and one to backfill the existing objects.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/408/1*PidMTIYHEnnnJD513DJdrw.png\" /></figure><p>We chose a passive replication strategy for the objects; the service would come through and clean up from time to time. A more active strategy, like a 3-phase commit, introduced risk into the upload phase. For instance, bugs or provider issues could slow or disable uploads completely.</p><p>It is also a simpler change in our system as it didn’t impact the request chain at all. We could iterate on the service with minimal risk to the primary flow. There is only a small fraction of objects that aren’t replicated in the event that we swap providers. Objects are available across providers and the system automatically handles that.</p><p><strong>Recovery</strong></p><p>The system is flexible enough to handle recovering quickly; we’d take degraded performance over an outage any day. Traffic can be quickly redirected internally by updating service configurations. We can direct traffic to any of our clouds, including splitting traffic, in order to keep from dropping requests. Of course there are cost and performance considerations when doing this, especially when crossing cloud boundaries, but uptime is the priority.</p><p>Just the other day we had to configure a cross cloud setup. We detected a latency issue in our Google integration and manually intervened to swap reads from S3 while we investigated. Once we worked out the kinks with the integration we returned the configuration to its resting state, preferring the cloud the service was in.</p><p>Doing things manually is often too slow. We needed the system to be smarter, preferring to serve the request even if it would be slow. To accomplish this, we made it so that our services all had fallbacks for each provider. The service could look at the bitmask of the content it was serving and then try each of the providers by priority. If the node was in Google, it would prefer GCS, but in the event of an error it would automatically load that from S3 or Cloudfiles as appropriate.</p><p><strong>Database considerations</strong></p><p>As with most companies, our database sizes keep going up. In Rackspace we had to manually create LVMs and rehydrate the instance. Doing this caused one of our secondary nodes to be unavailable for around a day. We run five fully-replicated nodes spanning two providers, meaning it wasn’t an unsafe procedure, it was a unnecessary risk. Now in Google and Amazon, we can provision much larger disks, as well as do quick resizing of the attached disks. We don’t have to completely destroy the node, just suspend, resize, and start it.</p><p>One of our main worries in the migration was doing the database step down. We had a chicken and egg situation: either we move the origin traffic then the database master or vice versa. In either case we would increased the latency-per-query. We do around 1500 req/sec and if we introduce extra latency by spanning providers, we could put enough back pressure on the system to start dropping requests. It would only be a few minutes while traffic swapped over, but it was a definite risk. To compensate, we made sure that we were at a low point in traffic, that we had plenty of capacity to swap over, and that were quick about it. We ended up choosing to move the database master then origin traffic. Ultimately, it was ultimately not a concern. The query times jumped up but not enough to impact serving traffic.</p><p><strong>Testing for a full migration</strong></p><p>After we updated all of the services we could to be cloud agnostic, we started testing. Lots of it. We needed to test all the ways that people interact with the systems that touch a provider, like uploading assets and deploying sites. We tried to be exhaustive because we knew that the first cutover would be risky — it was picking everything up and shifting it. If we missed one hard-coded path it could have tanked the whole migration.</p><p>The driving question of the testing phase was, “How will we know when it <em>isn’t</em> working?” That drove us to create a document with all the outage scenarios we could come up with. We made sure we had the monitoring in place to detect errors, as well as code that should handle the event. Once we had a reasonable unit test suite, we spun up all the boxes and services in the request chain as if they were production. We started to run sample requests through those services to verify that nothing was being dropped and timings were acceptable. We found some gotchas and quickly iterated on fixing those.</p><p>Once confident that our staging area was working right, we started to send it a small trickle of production traffic. We carefully watched all the charts and monitored all the incoming requests possible, making sure that none of them were dropped or stalled. We did this in each provider — carefully verifying that the services were preferring the right providers, that response times were roughly the same as the actual production system, and that no service got backed up from increased latencies. Now it was just a matter of actually picking up the origin and migrating it to another cloud provider.</p><p><strong>Actually pushing the button</strong></p><p>Now that the whole platform team was confident that Netlify’s origin servers were sufficiently agnostic, we had to do the actual cutover. The steps to the task were:</p><ol><li>Spin up the new capacity in the secondary provider.</li><li>Fail the database primary over to the new provider.</li><li>Update the entries in our service discovery layer.</li></ol><p>If successful, all the edge nodes in the CDN would automatically start to send traffic to the new nodes as the value rolls out. Each of the edge nodes would start to prioritize instances that are in the same provider and traffic would flow uninterrupted.</p><p>We chose a Sunday night to do the actual migration because that’s when Netlify sees its lowest traffic levels. The whole platform team got on a hangout, loaded up our dashboards, and prepared to aggressively monitor for errors or degraded performance that might result from the steps to come. Then we started pushing the buttons. We narrated each step, coordinating, and talking about any issues we saw. Thanks in large part to the planning and testing that went into the migration, the night was mostly spent chatting while the migration went off without a hitch.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/792/1*5FwlaEjKfDimHSAW-Z6MTA.png\" /></figure><p>As a result of the migration, we can now swap between different cloud providers without any user impact. This includes the databases, web servers, API servers, and object replication. We can easily move the entire brains of our service between Google, Amazon, and Rackspace in around 10 minutes with no service interruptions. Side note: Netflix’s world-class engineering team recently got failovers down from nearly an hour to <a href=\"https://medium.com/netflix-techblog/project-nimble-region-evacuation-reimagined-d0d0568254d4\">less than 10 minutes</a>. We’re going to work our tails off to beat their record.</p><p><strong>What comes next?</strong></p><p>We have tested that we can fail over our origins to the other providers as well, just not done the full failover. Going forward we are going to be adding more monitoring (as always on a platform team). We also set up a live standby in the other providers that we will be sending a trickle of real traffic to in order to make sure it is always ready. Then we are going to be taking steps to work to reduce the amount of time it takes for us to switch — our goal is under a minute — and remove any manual steps. As we refine the process and add more insights we think that we can reach that goal.</p><p>Every investment in our infrastructure introduces some level of risk and this project was fraught with it. I joined Netlify two years ago and we’ve talked about the importance of this project almost weekly since then. In that same time, it became increasingly clear that our current cloud provider could not offer us the quality of service we want to provide to our users. To me, this was a huge accomplishment and a great example of what a good team like ours can do. If you’re looking for a home on a platform team that’s doing exciting work, <a href=\"https://www.netlify.com/careers/?utm_source=medium&amp;utm_medium=blog&amp;utm_content=migration\">we’re hiring</a>. Let’s have a chat or a coffee.</p><p>This post was original published on the <a href=\"https://www.netlify.com/blog/2018/05/14/how-netlify-migrated-to-a-fully-multi-cloud-infrastructure/?utm_source=medium&amp;utm_medium=blog&amp;utm_content=migration\">Netlify blog</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cfca95bbbb0f\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/how-netlify-migrated-to-a-fully-multi-cloud-infrastructure-cfca95bbbb0f\">How Netlify migrated to a fully multi-cloud infrastructure</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"how-netlify-migrated-to-a-fully-multi-cloud-infrastructure-cfca95bbbb0f"},{"title":["Webserverless"],"link":["https://medium.com/netlify/webserverless-32f117e35996?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/32f117e35996","$":{"isPermaLink":"false"}}],"category":["jamstack","web-development","static-site-generator","development"],"dc:creator":["Phil Hawksworth"],"pubDate":["Tue, 10 Apr 2018 20:06:02 GMT"],"atom:updated":["2018-04-11T18:19:27.861Z"],"content:encoded":["<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8U_I_kTZf45mjloaHKFIFQ.jpeg\" /></figure><p>Today I saw a question on twitter which I have seen a few times before. It was posed by the rather excellent and lovely Drew McLellan.</p><style>body[data-twttr-rendered=\"true\"] {background-color: transparent;}.twitter-tweet {margin: auto !important;}</style><blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-align=\"center\" data-dnt=\"true\"><p>What do you think the best use cases are for a static site generator?</p><p>&#x200a;&mdash;&#x200a;<a href=\"https://twitter.com/drewm/status/983634762433810432\">@drewm</a></p></blockquote><script src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><script>function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height); resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === \"#amp=1\" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: \"amp\", type: \"embed-size\", height: height}, \"*\");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}twttr.events.bind('rendered', function (event) {notifyResize();}); twttr.events.bind('resize', function (event) {notifyResize();});</script><script>if (parent && parent._resizeIframe) {var maxWidth = parseInt(window.frameElement.getAttribute(\"width\")); if ( 500  < maxWidth) {window.frameElement.setAttribute(\"width\", \"500\");}}</script><p>There were some interesting responses, but this cheeky and playful response from the irrepressible Bruce Lawson jumped out at me:</p><style>body[data-twttr-rendered=\"true\"] {background-color: transparent;}.twitter-tweet {margin: auto !important;}</style><blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-align=\"center\" data-dnt=\"true\"><p><a href=\"http://twitter.com/drewm\" target=\"_blank\" title=\"Twitter profile for @drewm\">@drewm</a> making static sites?</p><p>&#x200a;&mdash;&#x200a;<a href=\"https://twitter.com/brucel/status/983634971956056064\">@brucel</a></p></blockquote><script src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><script>function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height); resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === \"#amp=1\" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: \"amp\", type: \"embed-size\", height: height}, \"*\");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}twttr.events.bind('rendered', function (event) {notifyResize();}); twttr.events.bind('resize', function (event) {notifyResize();});</script><script>if (parent && parent._resizeIframe) {var maxWidth = parseInt(window.frameElement.getAttribute(\"width\")); if ( 500  < maxWidth) {window.frameElement.setAttribute(\"width\", \"500\");}}</script><p>Thanks Bruce. Problem solved.</p><p>Bruce has a knack of cutting to the chase, and this made me smile. But it nudged at one of my trigger phrases, which Drew’s follow up question then prodded further:</p><style>body[data-twttr-rendered=\"true\"] {background-color: transparent;}.twitter-tweet {margin: auto !important;}</style><blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-align=\"center\" data-dnt=\"true\"><p><a href=\"http://twitter.com/brucel\" target=\"_blank\" title=\"Twitter profile for @brucel\">@brucel</a> Ok, so when/why would you make a static site?</p><p>&#x200a;&mdash;&#x200a;<a href=\"https://twitter.com/drewm/status/983635104286281729\">@drewm</a></p></blockquote><script src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><script>function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height); resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === \"#amp=1\" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: \"amp\", type: \"embed-size\", height: height}, \"*\");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}twttr.events.bind('rendered', function (event) {notifyResize();}); twttr.events.bind('resize', function (event) {notifyResize();});</script><script>if (parent && parent._resizeIframe) {var maxWidth = parseInt(window.frameElement.getAttribute(\"width\")); if ( 500  < maxWidth) {window.frameElement.setAttribute(\"width\", \"500\");}}</script><h4>A static site</h4><p>The term <em>static site </em>strikes me as being widely misunderstood. It is taken to describe the experiential characteristics of a site, and not, as probably intended, the attributes of the site architecture which delivers it.</p><p>It’s like describing <em>a blue site</em> or <em>a French site</em>.</p><p>In fairness, we know that naming things in computer science is hard, and as the tooling and processes around static site <em>architectures</em> have evolved, the results no longer need to be sites which are… well.. static.</p><p>A number of people replied to Drew’s follow up question by offering the scenarios of brochure sites and sites which have to change very infrequently. In other words, sites which behave statically.</p><p>Thomas Fuchs got closer to what my stock response to this question would be when <a href=\"https://twitter.com/thomasfuchs/status/983647675227824130\">he offered</a> things like improved security, performance and portability. All excellent and valuable points. But I still find myself trying to articulate this a little differently.</p><h4>Removing the web server</h4><p>There have been attempts to reset the static site misnomer before. The term <a href=\"https://www.jamstack.org\">JAMstack</a> has surfaced in an effort to more conveniently describe sites which can be served as static assets, and if needs be, enriched further via progressive enhancement and client-side access to other services. I talked to Chris and Dave about that recently on their <a href=\"https://shoptalkshow.com/episodes/303-jam-stack-phil-hawksworth/)\">Shop Talk Show</a> podcast.</p><p>I think of it like this:</p><p>Web servers can be difficult to secure, to scale, to optimize. When we operate or maintain our own web servers we need to ensure that they are resilient, that they are not single points of failure, that they service requests quickly under load (including all database queries and dynamic view population behind the scenes), and that they stay secure, updated, patched and healthy.</p><p>When we introduce important things like Content Management Systems (CMS) to the mix, our technical stack can get even more complex and subject to risk of attack or poor performance.</p><p>Back in 2012, when I was working at an <a href=\"https://www.rga.com\">agency</a> and exploring <a href=\"https://grabaperch.com/\">Perch CMS</a> (a self-hosted, <a href=\"https://en.wikipedia.org/wiki/LAMP_(software_bundle)\">LAMP stack</a> CMS, which was breathing some much needed fresh air into the state of content management systems) I asked Drew:</p><blockquote><em>Can you use Perch to generate static files to serve from a dumb server? …[I’m] trying to win arguments with talk of vastly reduced hosting infrastructure.</em></blockquote><p>Drew’s suggestion was to add a caching layer to my stack with something like <a href=\"https://varnish-cache.org/\">Varnish</a>. That’s a reasonable and popular suggestion, but not quite what I was looking for. I wanted to take moving parts <em>out</em> of the stack which served my visitors, not add to it if I could avoid it.</p><p>Doing this at scale can be challenging. Strategies to make sites more resilient often involve taking steps which emulate serving static assets anyway. Caching technologies and Content Delivery Networks (CDN) operate by taking what a web server generates dynamically (or Just In Time) and making those resources static and caching them for distribution and repeated use.</p><blockquote><em>Caching management and invalidation is not trivial.</em></blockquote><blockquote><em>Distributed CDN management is not trivial.</em></blockquote><blockquote><em>Web server management is not trivial.</em></blockquote><p>I want to focus on creating web sites which deliver beautiful experiences. I don’t want to design and manage infrastructure which addresses commoditized problems.</p><p>So, if I can build sites in a way that lets me cut out some of this complexity and risk?… If I can build a site in a way which, by default, is perfectly suited to deploying across distributed content delivery networks ?…</p><p>I’ll choose that.</p><p><a href=\"http://staticgen.com\">Static site generators</a> are tools for creating sites which are <em>capable</em> of being served predictably and reliably from a CDN. Whether those sites are brochure sites which changes infrequently, blogs like this this one, or large publishing sites like <a href=\"https://www.netlify.com/blog/2017/03/16/smashing-magazine-just-got-10x-faster/\">Smashing Magazine</a> complete with comments, subscriptions and e-commerce.</p><p>Having a site which is <em>capable</em> of being served without a web server offers huge advantages.</p><p>I approach the technical design of each site I work on, not by asking “Can I make this static?”, but by asking, “Can I avoid introducing the overhead of a web server?”. Sometimes the answer is no. Usually the answer is yes.</p><p>Where possible, I’m going webserverless.</p><p><em>This article was originally published at </em><a href=\"https://www.hawksworx.com/blog/webserverless\"><em>https://www.hawksworx.com/blog/webserverless</em></a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=32f117e35996\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/webserverless-32f117e35996\">Webserverless</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"webserverless-32f117e35996"},{"title":["Creating your own URL shortener with Netlify Forms and Functions"],"link":["https://medium.com/netlify/creating-your-own-url-shortener-with-netlify-forms-and-functions-a4286f55ea6c?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/a4286f55ea6c","$":{"isPermaLink":"false"}}],"category":["jamstack","web-hosting","web-development","serverless","aws"],"dc:creator":["Phil Hawksworth"],"pubDate":["Wed, 21 Mar 2018 14:31:15 GMT"],"atom:updated":["2018-03-22T15:56:42.491Z"],"content:encoded":["<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Cf-rsnrwB6GlhH8OLEv3LA.png\" /></figure><p>Now that Netlify’s support for <a href=\"https://www.netlify.com/blog/2018/03/20/netlifys-aws-lambda-functions-bring-the-backend-to-your-frontend-workflow/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=url-shortener\">Functions and Forms is officially out of beta</a>, we can all enjoy using them to add all sorts of capabilities to our JAMstack sites. Once you start exploring what can be done by using theses features, your imagination can really go wild, but its useful to first understand what they are and how you can use them.</p><p>This post looks at an example I built to show how you can use these capabilities to create a URL shortener for your own domain.</p><h3>Why would you want a URL shortener?</h3><p>There are lots of URL shorteners out there. Chances are you’ve used one in the past in order to turn some long, unfriendly looking URL into something memorable or short enough to type, dictate, or just for an easier copy and paste.</p><p>Well, this is another one of those. But instead of it being hosted by somebody else, this is an example of one that you could host for yourself.</p><p>Most of the time, the existing solutions are just fine. There are many to choose from — <a href=\"https://tinyurl.com\">https://tinyurl.com</a>, <a href=\"https://is.gd\">https://is.gd</a>, <a href=\"https://bit.ly\">https://bit.ly</a>, <a href=\"https://goo.gl\">https://goo.gl</a> and more. Some offer more features than others, with things like custom short codes and analytics. But all of them depend on a third-party service. They also all introduce an extra database query and DNS hop to the journey made when following a link. And, they typically create links which are not related to your own domain, which could be a nice facility to have.</p><p>This example is partly me just scratching an itch to have my own URL shortener which does not depend on an external service. I want the ability to easily make my own short URLs which won’t go dead if some service outside of my control gets retired. I also want to be able to have recognizable, branded URLs on my own domain.</p><h3>What does this implementation give me?</h3><p>The result will be a tool to quickly create new short URLs based on my own domain. It will have a simple interface so that I don’t need to touch any code to create new short URLs.</p><p>You can see what the end results look like and give the URL shortener a whirl on this demo site. But remember, this is a demo, so the links you create there might not last forever.</p><blockquote><a href=\"https://linkylinky.netlify.com\"><em>https://linkylinky.netlify.com</em></a></blockquote><p>If you’d rather go further with your experimentation, and deploy your own version to play with, you can do that with just a couple of clicks.</p><blockquote>Try it: <a href=\"https://app.netlify.com/start/deploy?repository=https://github.com/philhawksworth/linkylinky\">Deploy your own version to Netlify (for free)</a></blockquote><h3>How does this work?</h3><p>With this new <a href=\"https://linkylinky.netlify.com\">URL shortener site</a>, I’ve been automating something I’d been doing manually for some time. During conference talks I often include a link to my slides or other resources. I like the link to be meaningful and short to help people find their way to a URL that’s too long to jot down during a talk.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*oIEy3wNv-Z-5aelL.png\" /><figcaption>My typical “thank you” slide after a talk, with a convenient URL to reach my slides, wherever they may be.</figcaption></figure><p>To create this short link on my <a href=\"https://www.hawksworx.com\">own domain</a> I have been using a feature provided by Netlify, where I host my site. <a href=\"https://www.netlify.com?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=url-shortener\">Netlify</a> lets you specify redirect rules via a configuration file in your site’s code. You can use either a _redirects file, or add the configuration to Netlify’s global config file: netlify.toml.</p><p>For my own website, I prefer to use the netlify.toml option. The<a href=\"https://github.com/philhawksworth/hawksworx.com/commit/119b6c49c4fe42b9612a0f16f51c3380566ba75f\"> entry I added</a> to create the short URL above looked like this:</p><pre>[[redirects]]<br>  from = &quot;/talks/smashing&quot;<br>  to = &quot;https://speakerdeck.com/philhawksworth/next-wave-infrastructure&quot;<br>  status = 302</pre><p>These configurations have an effect on how traffic to your site is routed by Netlify’s CDN. I’ll not go into details of this here as I’ve <a href=\"https://medium.com/netlify/10-netlify-features-to-surprise-and-delight-225e846b7b21\">mentioned it before</a>. You can also get more detailed information directly from <a href=\"https://www.netlify.com/docs/redirects/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=url-shortener\">the docs</a>.</p><p>The key here though, is that this configuration sets up redirect rules which happen right on the CDN edge nodes. Traffic is not hitting a server or querying a database which I have to maintain or manage. It all happens at the node geographically closest to the site visitor, so it’s fast and resilient.</p><p>My website is already generated by a <a href=\"https://www.staticgen.com/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=url-shortener\">static site generator</a>. From here it feels like a short hop to have my build also generate the redirect rules for me and allow me to automate the process with a handy little UI.</p><p>That’s what I’ve done. <a href=\"https://linkylinky.netlify.com\">LinkyLinky</a> is an example of that, free from the noise of the rest of my blog.</p><h3>Step by step</h3><p><strong>1. Submitting new URLs to a form</strong></p><p>I want to be able to submit a new destination URL and receive a new shortcode URL in return. This is a static website so I don’t have a server to handle post requests, but thankfully there are several services around the JAMstack ecosystem which can offer form handling as a service. <a href=\"https://www.netlify.com/docs/form-handling/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=url-shortener\">Netlify has a form handling built</a> in, so I’ll just use that.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*NjbSC002I1qPz4NB.png\" /><figcaption>Form submissions are displayed you in your Netlify dashboard</figcaption></figure><p><strong>2. Generating the</strong><strong>_redirects configuration</strong></p><p>Since the submissions collected by Netlify’s Forms are also available programmatically <a href=\"https://www.netlify.com/docs/api/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=url-shortener\">via an API</a>, I can grab them when I run my build to use when populating the configuration file.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*LP4A4P-0LTzgwWIs.png\" /><figcaption>Creating a _redirects file as in a gulp task. (<a href=\"https://github.com/philhawksworth/linkylinky/blob/d273c5d407c61d3b1d9f89c67cb767661bf54a09/gulpfile.js#L74-L113\">Code can be found in the repo</a>)</figcaption></figure><p>This gives me an updated _redirects file for my site deployed to the CDN whenever my build runs, which looks a little like this:</p><pre>/66XXMgJR  https://news.bbc.co.uk  302<br>/MjwwEm0G  https://paper.dropbox.com/  302<br>/nZxxzDkD  https://www.whosampled.com/Dorothy-Ashby/sampled/  302<br>/j2ZZ0r2z  https://twitter.com/i/notifications  302<br>...</pre><p><strong>3. Adding some intelligence with Lambdas</strong></p><p>At this stage I have the basics of a working solution, but until I add some more logic, it’s not something I could confidently use. So far, the redirect rules generated are entirely based on the information posted to the form. That’s fine, but it doesn’t automatically create a shortcode for me. Nor does it avoid duplication of destination URLs or shortcodes. To do that, I’ll need the ability to introduce some logic at the time when the user is asking for a new redirect rule.</p><p>This is the perfect example of a something that can be achieved by using an AWS Lambda function, or similar <em>Function-as-a-Service</em> function from another provider like Google Cloud or Microsoft Azure.</p><p>Happily, Netlify now makes the development of AWS Lambda functions trivially easy thanks to Netlify Functions. There is great documentation and guidance on how to use Netlify Functions so I’ll skip to describing what my functions do.</p><p>Rather than posting my form data directly to the Netlify form handlers, I use JavaScript to post the request for a new short url to a function I created called generate-route. When this function is invoked via an HTTP request it can:</p><ul><li>Sanitize the submitted URL to ensure it includes a protocol and is well formed</li><li>Generate a unique shortcode</li><li>Ensure that this new entry would not create a duplicate in our _redirects file</li><li>Post the massaged data to my routes form</li><li>Return a message to the UI to tell the user what their new URL will be</li></ul><p>With the nicely formatted data posted to my form, I pick up where I was before, triggering a build and deployment automatically.</p><p>Meanwhile, I’ve told the user what their new URL will be.</p><p>One of the many benefits of using Netlify Functions to create and manage Lambdas is that this code is all managed and deployed from within the codebase of a site. No fiddly separate development workflows or deployment processes to update my Lambda functions for me. You can see this function in a bit more detail if you take a <a href=\"https://github.com/philhawksworth/linkylinky/blob/master/src/lambda/generate-route.js\">look at it in my site repo</a>.</p><p><strong>4. Shortcutting the build time by accessing the Forms API directly</strong></p><p>Once small problem is still lingering.</p><p>The new redirect will not be active until my build and deployment to the CDN completes. For this project, that process is averaging around 25 seconds, which is nice and fast for a site build and deployment, but an eternity for a user to resist the temptation to click on the shiny new link that I just gave them.</p><p>I’ll need to make that work for them <em>even before</em> the build completes.</p><p>And I can do that by cheating.</p><p>Although the build takes a few seconds, the data posted to the form is available instantly via the API. I am already calling that API during the build process. If I called the same API directly from the client via JavaScript, it could look up the new route and then redirect the user to the correct destination URL right away. No more waiting for the build to finish.</p><p>Once again, this is a great moment to use a Lambda function. On this occasion the <a href=\"https://github.com/philhawksworth/linkylinky/blob/master/src/lambda/get-route.js\">get-route</a> Lambda function acts a bit of middleware to proxy requests from the browser to the form submission API.</p><p>To see what that function returns you can try calling it by following the link below. It should tell you were a given shortcode will resolve to.</p><blockquote><a href=\"https://linkylinky.netlify.com/.netlify/functions/get-route?code=wjVZ9GXX\">https://linkylinky.netlify.com/.netlify/functions/get-route?code=wjVZ9GXX</a></blockquote><p>Here’s why this is preferable to just hitting the API directly from the browser:</p><ul><li>Calls to the form submissions API need to be authenticated. I don’t want to expose my auth token by including it in the code or show it in requests made from the client to the API. I’ll keep that between the Lambda and the API thanks to<a href=\"https://www.netlify.com/docs/continuous-deployment/#build-environment-variables?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=url-shortener\"> Netlify’s support for environment variables</a></li><li>The form submissions API only gives us basic access to the submissions content. I need somewhere to run the logic to search for the URL which corresponds to a given shortcode. That lookup happens in this Lambda function.</li></ul><p>Combining these pieces gives me just what I need. Now I can quickly make new short urls on my own domain without any fuss.</p><h3>Big opportunities from common building blocks</h3><p>This is only a proof of concept, but it is already useful for me. I’ll most likely add a few enhancements like the ability to provide a specific, meaningful shortcode rather than have the system generate a random one for me. And perhaps I might sometimes want short urls as a temporary resource, so an option to have my short urls expire after a given time could be handy.</p><p>With these building blocks there are all sorts of possibilities. The simple API I’ve created could be called from anything that <em>speaks HTTP</em>. Perhaps I’ll make it available through a Slack integration or and Alfred App action</p><p>Access to utilize APIs, trigger automated build processes via incoming webhooks, post to external webhooks, and execute Lambda functions, all combine to create so many possibilities. My imagination is bristling with ideas of things I could make from what are seemingly simple building blocks.</p><p><a href=\"https://www.netlify.com?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=url-shortener\">What might you make?</a></p><p><em>(This slightly expanded version was </em><a href=\"https://www.netlify.com/blog/2018/03/19/create-your-own-url-shortener-with-netlifys-forms-and-functions/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=url-shortener\"><em>originally posted on Netlify’s blog</em></a><em>)</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a4286f55ea6c\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/creating-your-own-url-shortener-with-netlify-forms-and-functions-a4286f55ea6c\">Creating your own URL shortener with Netlify Forms and Functions</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"creating-your-own-url-shortener-with-netlify-forms-and-functions-a4286f55ea6c"},{"title":["Comparing modern build tools"],"link":["https://medium.com/netlify/comparing-modern-build-tools-8eca722b69b6?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/8eca722b69b6","$":{"isPermaLink":"false"}}],"category":["javascript","jamstack","tools","web-development"],"dc:creator":["Brian Douglas"],"pubDate":["Wed, 14 Mar 2018 10:10:59 GMT"],"atom:updated":["2018-03-14T10:10:59.322Z"],"content:encoded":["<p>Build tools have become a necessary component in the workflow for modern web applications. I have previously covered the basics of <a href=\"https://www.netlify.com/blog/2017/11/15/what-build-tools-can-do-for-you/\">what build tools can do for you</a> to show how builds help with scripting, automation, and eliminating complexity. Taking those into consideration, I’m going to provide a closer look at some of the more popular build tools and how they might make sense for your projects.</p><p>This post isn’t meant to be an exhaustive list. Rather, it’s meant to provide suggestions to help you get started in researching how different build tools improve the development workflow. While reading through this comparison on build tools you will gain some knowledge to better equip yourself for comparing how these tools line up with your specific needs.</p><h3>What are we comparing again?</h3><p>This guide will only look at build tools that have the ability to perform project module bundling, which is the process of stitching dynamic content into modules and bundles into static assets files. This process can be enhanced using scripting, automation, and minification. Not all build tools are created equal and not all build tools can do all of the above. Some tools require more configuration and some are drop-in solutions that do most of what you need out of the box.</p><p>It’s likely you’ll have (or develop) a preference for how much you’d like a build tool to do. So rather than choosing one build tool to rule them, this post will cover the advantages, gotchas, and an ideal project profile for each tool.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*jK6IgyKgztowMxez7PMLhw.png\" /></figure><h4>Browserify</h4><p>Advantage: Drop dead simple<br>The catch: Development on the project has slowed down a lot <br>Ideal project: Projects that are looking to move away heavy usage of script tags and move CommonJS requires instead.</p><p><a href=\"http://browserify.org/\">Browserify</a> focuses on the simplicity of getting started and it is a great introduction to module bundling in JavaScript development. Browserify originally came into existence as a way to allow front-end developers to use <a href=\"http://requirejs.org/docs/commonjs.html\">CommonJS</a> (require statements) in the browser the same way you would in the server render <a href=\"https://nodejs.org/en/\">node</a> application. Previously web development used multiple script tags on the page to build modern web apps. This tool browserifies all your JavaScript files into a concatenated (combined and merged) file that can easily be included on the page as a single script tag.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/499/1*lTG9HcDDN5HqzAUEaaPE2g.png\" /></figure><p>Using Browserify starts with the installation of the CLI. I recommend using npm from the command line.</p><pre> npm install browserify</pre><p>Once installed, you can point your JavaScript entry point of your application (most likely your index.js) to a location to start the Browserifying process.</p><pre> browserify index.js &gt; bundle.js</pre><p>The result is a bundled version of your JavaScript that can be included in your index.html.</p><pre><br> &lt;script src=”bundle.js”&gt;&lt;/script&gt;</pre><p>Browserify implementation is feature complete and focuses on JavaScript improvements out of the box. To support the bundling of non-JavaScript assets, like CSS or images, there is a healthy list community-created <a href=\"https://github.com/browserify/browserify/wiki/list-of-transforms\">transforms</a> (all named with ify endings, how clever) to source for those solutions. I’m a huge fan of enlisting the open source community to take a project further, but if you’re giving Browserify a try take heed: some transforms haven’t received new updates in more than six months. That being said there is plenty of areas to contribute back to the community by providing plugins to your project’s solutions.</p><p>Unlike some other build tools, Browserify doesn’t have a standard config file to maintain. however you can leverage the node <a href=\"https://github.com/browserify/browserify#packagejson\">package.json to handle more advance configurations</a>. The work of Browserify is inferred through the plugins and what is inside your JavaScript files. For projects that do not constantly need to be updated, this can be a beautiful thing. For projects in need of a lot of tools and optimizations, the missing config file can become a burden because there is no source of truth or place to expose the witchcraft of the build tool.</p><p>Consult the Browserify documentation as well as the l<a href=\"https://codeutopia.net/blog/2016/01/25/getting-started-with-npm-and-browserify-in-a-react-project\">ist of transforms</a> to see it includes everything you need to make your development workflow happy. You can also use this tutorial on <a href=\"https://codeutopia.net/blog/2016/01/25/getting-started-with-npm-and-browserify-in-a-react-project/\">how to build a React app with Browserify</a> to see it in action. If simple is what you need then Browserify is something I would consider for your next project.</p><h4>Webpack</h4><p>Advantage: Actively-supported project with tons of features out of the box<br>The catch: Takes a bit of custom configuration to get right<br>Ideal project: Projects that are looking to stay up to date with latest and greatest changes. Projects also looking to do code splitting should consider webpack as well.</p><p><a href=\"https://webpack.js.org/\">Webpack</a> is a build tool that is built on 4 main concepts: Entry, Output, Plugins, and Loaders. Once you understand <a href=\"https://www.netlify.com/blog/2017/01/03/4-key-concepts-of-webpack/\">the ideas around these concepts</a>, you can get Webpack up and running on a project. Webpack took can feel similar to Browserify in some ways with enhanced features through a <a href=\"https://webpack.js.org/plugins/\">community of plugins</a>. Webpack however, comes with more features out of the box with plans to continue to add more and continually <a href=\"https://medium.com/webpack/webpack-4-changes-part-1-week-24-25-fd4d77674e55\">rethinking the design</a> of the project.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gdoQ1_5OID90wf1eLTFvWw.png\" /></figure><p>I wrote previously wrote a guide for getting started with [Webpack from scratch](<a href=\"https://www.netlify.com/blog/2017/11/30/starting-with-webpack-from-scratch/\">https://www.netlify.com/blog/2017/11/30/starting-with-webpack-from-scratch/</a>) and it focusing on the leveraging the Webpack CLI to build a React application. Webpack requires you create a separate config file to support your Webpack build efforts. This file is nothing more than a JavaScript object that Webpack uses to enable and disable features during the build process based on keys and values within the config object.</p><pre><br> // example of a webpack.config.js<br> <br> module.exports = {<br>   entry:’./index.js’,<br>   output: {<br>   filename: ‘bundle.js’<br> }</pre><p>Within the config you can identify the entry point of your project as well as the location of where you would like to place your bundle. This makes running the Webpack build simpler since do not need remember specific commands, you just `webpack` to create you build.</p><pre><br> npm install webpack<br> webpack<br></pre><p>The Webpack config can be a sweet way to approach adding new features and tools to enhance your build process, but like most sweets things, a few additions here and there can cause your config to bloat into to a form that unmanageable. A config that looks unmanageable can be a form where the development team on a project avoids changing or updating the Webpack config for fear of breaking the build due to one too many added to the Webpack config file.</p><p>The React team has solved this Webpack problem by abstracting the config away into a hidden script beneath the create-react-app CLI tool. If you take a look at <a href=\"https://github.com/facebook/create-react-app/blob/master/packages/react-scripts/config/webpack.config.dev.js\">the hidden config</a>, the file has some of the best laid out comments you may ever have seen in a config, but the fact that it needs so many comments makes you question if there is a better way to have such fine tuned configuration without the needed walls of comments to support each decision.</p><pre>// excerpt from the creat-react-app&#39;s webpack config<br><br>    module.exports = {<br>      // You may want &#39;eval&#39; instead if you prefer to see the compiled output in DevTools.<br>      // See the discussion in https://github.com/facebookincubator/create-react-app/issues/343.<br>      devtool: &#39;cheap-module-source-map&#39;,<br>      // These are the &quot;entry points&quot; to our application.<br>      // This means they will be the &quot;root&quot; imports that are included in JS bundle.<br>      // The first two entry points enable &quot;hot&quot; CSS and auto-refreshes for JS.<br>      entry: [<br>        // We ship a few polyfills by default:<br>        require.resolve(&#39;./polyfills&#39;),<br>        // Include an alternative client for WebpackDevServer. A client&#39;s job is to<br>        // connect to WebpackDevServer by a socket and get notified about changes.<br>        // When you save a file, the client will either apply hot updates (in case<br>        // of CSS changes), or refresh the page (in case of JS changes). When you<br>        // make a syntax error, this client will display a syntax error overlay.<br>        // Note: instead of the default WebpackDevServer client, we use a custom one<br>        // to bring better experience for Create React App users. You can replace<br>        // the line below with these two lines if you prefer the stock client:<br>        // require.resolve(&#39;webpack-dev-server/client&#39;) + &#39;?/&#39;,<br>        // require.resolve(&#39;webpack/hot/dev-server&#39;),<br>        require.resolve(&#39;react-dev-utils/webpackHotDevClient&#39;),<br>        // Finally, this is your app&#39;s code:<br>        paths.appIndexJs,<br>        // We include the app code last so that if there is a runtime error during<br>        // initialization, it doesn&#39;t blow up the WebpackDevServer client, and<br>        // changing JS code would still trigger a refresh.<br>      ],<br>      output: {<br>        // Add /* filename */ comments to generated require()s in the output.<br>        pathinfo: true,<br>        // This does not produce a real file. It&#39;s just the virtual path that is<br>        // served by WebpackDevServer in development. This is the JS bundle<br>        // containing code from all our entry points, and the Webpack runtime.<br>        filename: &#39;static/js/bundle.js&#39;,<br>        // There are also additional JS chunk files if you use code splitting.<br>        chunkFilename: &#39;static/js/[name].chunk.js&#39;,<br>        // This is the URL that app is served from. We use &quot;/&quot; in development.<br>        publicPath: publicPath,<br>        // Point sourcemap entries to original disk location (format as URL on Windows)<br>        devtoolModuleFilenameTemplate: info =&gt;<br>          path.resolve(info.absoluteResourcePath).replace(/\\\\/g, &#39;/&#39;),<br>      },<br><br>      // ... there is so much more to this</pre><p>The Webpack team is actively developing this project and doing their part to clear up the confusion around the config. A lot of the missing features that once needed a Webpack plugin are now included in the library itself, including tree-shaking, uglifying, and even web assembly (WASM) support. The <a href=\"https://webpack.js.org/concepts/\">well-written through documentation</a> also helps to make this Webpack as a build tool more approachable and has been consistently maintained since the launch of Webpack 2 (Fall of 2016).</p><p>Webpack not only has a focus on module bundling, it includes code splitting as a built-in feature Code splitting is the practice of loading only the content that’s needed, when it is needed by leveraging separate page split bundles base usually on routing. This has the potentially to improve page load speed and the overall browsing experience. Code splitting does however come with a learning curver, one that I have personally not fully learned, but the Webpack team members are trying their best to flatten that curve with <a href=\"https://webpack.academy/\">webpack.academy</a>.</p><p>There are lots of community-built Webpack config boilerplates, including a pretty neat tool called <a href=\"https://www.webpackbin.com/\">Webpackbin</a>. Webpackbin is a sandbox to build and configure Webpack examples. You can generate links from here which is nice when researching Webpack configurations, as authors tend to post their configs in the sandbox and provide the URL to share.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b5dxUonI92EP1IgYgvwOsw.png\" /></figure><p>Webpack is working towards being the batteries included, but some parts sold separately build tool. Webpack can handle almost every concern you have when b web applications these days, but you will also likely need to read the manual (<a href=\"https://webpack.js.org/concepts/\">documentation</a>) a lot to get it your build up and running to your liking.</p><h4>Rollup</h4><p>Advantage: Built-in features for package management<br>The catch: You’ll need to make sure your project has implemented the ES6 syntax<br>Ideal project: Projects looking to use slightly less configuration for the build process and already using the latest ESNext features like ES modules</p><p>Rollup is a module bundler for JavaScript which compiles small pieces of code into something larger and more complex. It uses the new the new version of JavaScript’s ES6 module system, instead of previous idiosyncratic solutions such as CommonJS and AMD, to perform the rollup (bundling) of your project. ES6 modules let you freely and seamlessly combine the most useful individual functions from your favorite libraries.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Am5hoqyrZvoJopjruvpfTg.jpeg\" /></figure><p>Getting started with Rollup can be done via the command line. Just point your index.js and provide a name for your bundled ouput.</p><pre>npm install -D rollup<br>rollup index.js — o bundle.js — f iife</pre><p>To save us from needing to constantly repeat the same commands, you have the option to add a rollup.config.js file, similar to what we saw in webpack. The same risks about config are just as valid in ght ejs</p><pre> // rollup.config.js<br> export default {<br>   input: &#39;src/index.js&#39;,<br>   output: {<br>   file: &#39;bundle.js&#39;,<br>   format: ‘cjs’<br> };<br></pre><p>Rollup has gained more popularity with package and open source maintainers, because of built in features for package management as oppose web applications. Some of the features mirror what you could do with <a href=\"https://github.com/umdjs/umd\">Universal Module Definition</a>’s(UMD) and make Rollup a great bridge between the JavaScript’s UMD need and ES6 modules. Because ES6 is the hotness Rollup does not work with CommonJS required files <a href=\"https://github.com/rollup/rollup-plugin-commonjs\">without a plugin</a>. This is only a limitation for older projects that have not yet implemented the ES6 syntax into their workflow. However if you are starting a new project, there is not much in the way of limitation for you.</p><p>As of the the Spring of 2017 all major browser support ES6 modules natively, which has Rollup now looking to claim a new competitive advantage as well. Rollup does come with native support native support for tree-shaking, with is the ability to remove unused code out of your project bundle, which you can see in this example from the <a href=\"https://rollupjs.org/repl\">rollup repl</a>. This is valuable for projects looking for assistance where they can get it with page optimizations.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/880/1*jn2g4AxhBjPYZ9aNWT7t4w.png\" /></figure><p>Though tree-shaking seems like a small feature, consider projects like <a href=\"https://momentjs.com/\">Momentjs</a> or <a href=\"https://lodash.com/\">lodash</a> which are massive projects to begin with. Tree-shaking provides the ability to exclude all the parts of the library out of your bundle and only include the portion of the library you are using.</p><p>There is a lot more you can rollup besides trees, so I encourage you consult the <a href=\"https://rollupjs.org/guide/en\">Rollup guides</a> for <a href=\"https://rollupjs.org/guide/en#tree-shaking\">more information</a> how you can leverage tree-shaking and other features in your next project.</p><h4>Parcel</h4><p>Advantage: Zero config needed <br>The catch: Newer project with less documentation to reference<br>Ideal project: Small projects and prototypes looking to get started quickly</p><p><a href=\"https://parceljs.org/\">Parcel</a> came out at the end 2017 and gets the privilege of wrapping all the JavaScript points of configuration fatigue in a tiny little package. Parcel removes the complication of build tools and works out of the box with the most popular plugins in the JavaScript space, including babel transforms.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5J9mh8tOdjCE3AB9220MuA.png\" /></figure><p>Similar to Browserify there’s also no config file, but there are also no Parcel-specific plugins. Parcel relies on existing JavaScript ecosystem projects like Babel to do the work. Parcel is only the orchestrator. You can also include Babel transforms and plugins in your package.json or .babelrc and Parcel will know to include it in the build process. There is no extra configuration needed which is a very intriguing feature. There is also no need to learn one more library to maintain your project (a.k.a the dreaded JavaScript fatigue).</p><p>Getting started with Parcel is similar to the others, but instead of providing an input and and output for the bundle, you just provide the entry in the script.</p><pre> npm install parcel-bundler<br> parcel build index.js</pre><p>Any other feature can be found in the documentation, but spoiler they require you to write modern JavaScript to do so. There is really no magic under the hood of this project. Take a look at this <a href=\"https://blog.vigneshm.com/building-a-reactjs-project-with-parceljs-d88cdd178e50\">example for getting a React application running using Parcel</a>. As mentioned this project is still fairly new but seems promising. There is already some great documentation available and a</p><h4>Microbundle</h4><p>Advantage: Zero config needed with an extremely small footprint<br>The catch: Newer project with less documentation to reference<br>Ideal project: Size conscious project looking to be shared as a plugin or addon for other projects</p><p>If you have not heard of Parcel, there is a chance you have not heard of <a href=\"https://github.com/developit/microbundle\">Microbundle</a>, the zero-configuration bundler for tiny modules. Microbundle is powered by the before-mentioned <a href=\"https://github.com/rollup/rollup\">Rollup</a> project and aims to take their module bundling to next level by removing the configuration step. Similar to Parcel, it will bundle your project using nothing but the package.json. So be sure to include all the necessary dependencies needed to run your JavaScript and bundle your assets.</p><pre> npm install microbundle<br> microbundle</pre><p>Microbundle will assume you have an index.js if no entry point is provided as an option. It will also create a bundle and minify that same file if no output is provided as well. Not only is bundled version created, a UMD version is also provided as part of the bundling process.</p><pre>// index.js<br>function () {<br>  console.log(“Hello World”)<br>}<br> <br>// microbundled into index.m.js<br>function O(){console.log(“FOOOOOOOOOO”)}O();<br> <br>// microbundled into index.umd.js<br>!function(e,n){“object”==typeof exports&amp;&amp;”undefined”!=typeof module?     n():”function”==typeof define&amp;&amp;define.amd?define(n):n()}(0,function(){console.log(“FOOOOOOOOOO”)});<br></pre><p>This could is useful for small shareable projects to be embedded into others. Thanks to this post I only now discovering this Microbundle but could see this being useful for the <a href=\"https://www.netlify.com/docs/identity/\">Netlify Identity Widget</a>, which is a project meant to be embedded into larger projects and already being bundled manually into a UMD.</p><h4>Now go build something</h4><p>No matter your programming style there is a solution for you and choosing a build tool comes down to what kind of control you want. My attempt was to provide a progression from no configuration to a lot configuration. Despite the amount of choices, all build tools work with <a href=\"https://www.netlify.com/\">Netlify</a> and can be a useful part of maintaining your application, so try a few and see which one works for you.</p><p>If you’re a big fan of a tool that was not listed here, please leave a comment and let me know.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8eca722b69b6\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/comparing-modern-build-tools-8eca722b69b6\">Comparing modern build tools</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"comparing-modern-build-tools-8eca722b69b6"},{"title":["How Netlify’s deploying and routing infrastructure works"],"link":["https://medium.com/netlify/how-netlifys-deploying-and-routing-infrastructure-works-c90adbde3b8d?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/c90adbde3b8d","$":{"isPermaLink":"false"}}],"category":["mapping","engineering","wardley-maps","infrastructure","computer-science"],"dc:creator":["David Calavera"],"pubDate":["Mon, 05 Mar 2018 19:00:28 GMT"],"atom:updated":["2018-03-30T19:17:58.627Z"],"content:encoded":["<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5n8tevsZPhFAnWnJTzO3lA.jpeg\" /><figcaption>Photo by <a href=\"https://unsplash.com/photos/IUY_3DvM__w?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Matt Duncan</a> on Unsplash</figcaption></figure><p>Every now and then, people ask about the advantages of using Netlify before a traditional cloud provider’s file storage service. Cloud providers can serve static files from your own domain name, so why use something else? In fact, Netlify uses those services to store files too. The difference is in the value added by Netlify on top of those services.</p><p>If we were going to look at this difference on a map, we’d say that file storage is a commodity. Storage costs are very low, which turns those services into utilities. Anyone can take several files and upload them to the cloud. This presents us with some opportunities that we’re going to explore.</p><p>The first one is about what to show your visitors when they use different domains and subdomains. For instance, I want to show specific content for www.example.com and beta.example.com. This is usually solved by custom solutions. You can make that work with the right combination of DNS records and storage buckets.</p><p>Another interesting opportunity is efficient file uploads. Off the shelf tools usually upload all your files to a storage bucket every time you run them, even if there are files that have not changed. It makes this process slower than it should be if they only uploaded the files that have changed. This was one key advantage for Dropbox. Their desktop clients knew how to manage files to save bandwidth and time.</p><p>Off the shelf tools don’t offer bucket integrity either. If two visitors request the same file while you’re updating your bucket’s content, they can get different content. If that content requires new CSS and JavaScript, your visitors won’t see what you expect them to see. You can work around this with the right combination of CDN caching, expiration headers, and scripts to expire the cache after each upload.</p><p>Netlify solves these problems for you, and many others. We’ve created a real product from the experience of implementing those custom built solutions ourselves. The rest of this post explains some infrastructure details behind Netlify’s deploying and routing mesh.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Z1ohYUjP4E0vQLmGRbAZDg.png\" /><figcaption>Netlify’s situational map</figcaption></figure><p>We’ve built Netlify’s core around <a href=\"https://en.wikipedia.org/wiki/Merkle_tree\">Merkle trees</a>, the same structures used in Git, ZFS file systems, and blockchain technology. A Merkle tree is a structure where each node is labeled with the cryptographic hash of the labels of all the nodes under it.</p><p>As you read earlier, we use cloud file storage to persist content. But, we use content addressable references rather than file names as identifiers. We don’t store files with the name jquery-3–31.js , we hash its content and we use that as the file name. This gives strong guarantees that we serve the same content regardless the file name. We don’t care if it’s named jquery-3–31.js and you decide to rename it later to jquery-3-latest-download.js. Each deploy calculates those hashes and generates a new tree based on the content that changed, and content that we already have stored. If you only change one file, we will only upload one file. If you rename a file, we will create a new node in the tree with that file name, without upload its content. If you remove a file, we won’t reference it in the new tree. That also means that each deploy is immutable. We always serve the contents of the same tree under a domain. When we finish processing new deploys, we only swap the tree to serve. Having immutable trees also prevents us from showing mixed content.</p><p>These immutable guarantees are also the base for many other features. The first one is <a href=\"https://www.netlify.com/docs/versioning-and-rollbacks/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=deploying-and-routing?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=deploying-and-routing\">atomic and instant rollbacks</a>. As Murphy said, if something can go wrong, it will go wrong at some point. Immutability doesn’t prevent you from publishing content with broken markup, and content that you publish on accident. Since your previous deploy was never modified, we can revert to that state at any time. We only need to change the tree reference where your domain points again.</p><p>Another interesting advantage of immutable deploys is that we can point any name you want to any content you want. This is the base for what we call <a href=\"https://www.netlify.com/blog/2016/07/20/introducing-deploy-previews-in-netlify?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=deploying-and-routing\">Deploy Previews</a> and <a href=\"https://www.netlify.com/docs/continuous-deployment/#deploy-contexts?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=deploying-and-routing\">Branch Deploys</a>. Given a set of changes in a Git branch, a DNS subdomain entry, and some basic routing rules, it’s not hard to route traffic to a specific tree in a safe and reliable way. We can serve different content in beta.example.com and www.example.com by pointing each domain to a different tree. We can also build on top of these primitives to implement new features, like <a href=\"https://www.netlify.com/blog/2018/03/02/how-to-use-split-tests-to-give-users-access-to-private-features/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=deploying-and-routing?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=deploying-and-routing\">Split Testing at the edge</a> by Git branch.</p><p>These are the foundations of why Netlify is different than using a cloud provider’s file service. We’ve taken an opportunity that those commodity services offer to build an awesome product.</p><p>If you’re excited about web and infrastructure engineering, we’re always looking to <a href=\"https://www.netlify.com/careers\">grow our team</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c90adbde3b8d\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/how-netlifys-deploying-and-routing-infrastructure-works-c90adbde3b8d\">How Netlify’s deploying and routing infrastructure works</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"how-netlifys-deploying-and-routing-infrastructure-works-c90adbde3b8d"},{"title":["JAMstack with Gatsby, Netlify and Netlify CMS"],"link":["https://medium.com/netlify/jamstack-with-gatsby-netlify-and-netlify-cms-a300735e2c5d?source=rss----7ed0459bbb88---4"],"guid":[{"_":"https://medium.com/p/a300735e2c5d","$":{"isPermaLink":"false"}}],"category":["jamstack","cms","netlify","gatsbyjs","static-site-generator"],"dc:creator":["Pedro Duarte"],"pubDate":["Thu, 01 Feb 2018 13:38:41 GMT"],"atom:updated":["2018-11-27T10:53:49.799Z"],"content:encoded":["<p>I’ve recently migrated <a href=\"https://donarita.co.uk\">Dona Rita’s</a> website from <a href=\"https://jekyllrb.com/\">Jekyll</a> to <a href=\"https://www.gatsbyjs.org/\">Gatsby</a>. I’d like to share my process with you, as well as a few tips about building a <a href=\"https://jamstack.org/\">JAMstack</a> site with <a href=\"https://gatsbyjs.org\">Gatsby</a>, <a href=\"https://www.netlify.com\">Netlify</a> and <a href=\"https://www.netlifycms.org\">Netlify CMS</a>.</p><h3>A bit of background</h3><h4>What is JAMstack?</h4><p><em>JAM</em> stands for <strong>J</strong>avascript <strong>A</strong>PI &amp; <strong>M</strong>arkup. The term JAMstack was popularised by <a href=\"https://twitter.com/biilmann?lang=en\">Mathias Biilmann</a> (CEO &amp; Co-founder of Netlify) to describe “a modern web development architecture based on client-side JavaScript, reusable APIs, and prebuilt Markup”. You can read more about <a href=\"https://jamstack.org\">JAMstack here</a>.</p><h4>What is Gatsby?</h4><p>Gatsby is a <em>blazing-fast</em> <strong>Static Site Generator for React</strong> — this is important because I wanted to keep the JAMStack I had with Jekyll. Apart from being a SSG, Gatsby is build with modern technologies, it’s future-proof, it’s a static PWA out-of-the-box and you can query your data from anywhere via GraphQL 😍</p><h4>What/Who is Dona Rita?</h4><p>Dona Rita is my side business. When I’m not busy coding, I’m selling gluten-free Brazilian cheese balls to the British population 😅</p><h3>My process</h3><p>The original Dona Rita website was built with Jekyll and hosted on Github Pages. It was built in an MVP approach, you can <a href=\"https://medium.com/@OiDonaRita/starting-a-food-business-from-scratch-9baed673657c\">read more about that here</a>. Overall, it was working well, but let’s be honest… Liquid partials are not React components.</p><p>When I decided to change my setup from Jekyll to Gatsby, I wasn’t quite sure how much work/effort that’d be. I wanted to keep things as simple as possible. In the end I think I was able to find a nice process.</p><p>Here’s how I did it:</p><h4>Documentation</h4><p>I started by reading <a href=\"https://www.gatsbyjs.org/docs/\">Gatsby’s documentation</a>. They also wrote a neat <a href=\"https://www.gatsbyjs.org/tutorial/\">tutorial</a>. Gatsby works differently than Jekyll, so it’s worth giving these a read first.</p><h4>Choosing a Gatsby starter</h4><p>I used the <a href=\"https://github.com/gatsbyjs/gatsby-starter-hello-world\"><em>gatsby-starter-hello-world</em></a>. This is probably the simplest starter you can use. I chose this because I wanted to start with bare minimum configuration. I find it easier to learn a framework this way.</p><h4>Creating a Gatsby Layout</h4><p>Anything you add to a <a href=\"https://www.gatsbyjs.org/docs/building-with-components/#layout-components\">Layout Component</a> will be shared amongst all other pages. This is particularly useful for global components, such as Navigation or Footer.</p><p>You can see the source code for my <a href=\"https://github.com/peduarte/dona-rita-website/blob/master/src/layouts/index.js\">layout here</a>.</p><h4>Stateless pages</h4><p>I started by creating stateless <a href=\"https://www.gatsbyjs.org/docs/building-with-components/#page-components\">Page Components</a>. The point here was to start getting a feel for how Gatsby creates pages, without having to worry about GraphQL.</p><p>I left stateful pages (pages that depend on data) and dynamic pages (pages that will be generated from Markdown files) for later.</p><p>The first page I added was the <a href=\"https://www.donarita.co.uk/ingredients/\">Ingredients</a> page, you can see the <a href=\"https://github.com/peduarte/dona-rita-website/blob/master/src/pages/ingredients.js\">source code here</a>.</p><h4><strong>Adding styles</strong></h4><p>The original Dona Rita website was styled with global CSS, written in Sass. To keep things moving, I decided to keep this approach instead of converting my styles to <a href=\"https://www.gatsbyjs.org/docs/styled-components/\">Styled Components</a>.</p><p>In order for Gatsby to process Sass files, you need install <a href=\"https://www.gatsbyjs.org/packages/gatsby-plugin-sass/\"><em>gatsby-plugin-sass</em></a><em> </em>(drop-in support, works like a charm).</p><p>Because I needed to inject the styles globally, I imported them in my Layout file.</p><h4>Stateful pages</h4><p>By now I had a few static pages rendered and styled correctly. I was becoming more comfortable with Gatsby and I was ready to create my first stateful page.</p><p>I decided to start with the <a href=\"https://donarita.co.uk/buy\">Buy Page</a>. For this page I needed to get access to a list of allowed postcodes which I will be passed down to my <a href=\"https://github.com/peduarte/dona-rita-website/blob/master/src/components/Product/Product.js#L26\">Product Component</a> to determine whether we can deliver there or not.</p><p>I created a file called <a href=\"https://github.com/peduarte/dona-rita-website/blob/master/src/data/postcodes.json\">postcodes.json</a> in a src/data/ directory and installed the following dependencies: <a href=\"https://www.gatsbyjs.org/packages/gatsby-source-filesystem/\"><em>gatsby-source-filesystem</em></a> and <a href=\"https://www.gatsbyjs.org/packages/gatsby-transformer-json/\"><em>gatsby-transformer-json</em></a><em>.</em></p><p><a href=\"https://github.com/peduarte/dona-rita-website/blob/master/src/data/postcodes.json\">Here’s source code</a> for the JSON file and <a href=\"https://github.com/peduarte/dona-rita-website/blob/master/src/pages/buy.js#L35-L41\">here’s the source code</a> for the Buy page.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RdyIu4xXNVB7LNS51p2fHQ.png\" /><figcaption>Postcode JSON file</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*18sWSQBIXn1_vmDUEOR4VA.png\" /><figcaption>Querying postocodes with GraphQL</figcaption></figure><p><strong>#protip:</strong> Gatsby has a <a href=\"https://www.gatsbyjs.org/tutorial/part-four/\">really nice tutorial</a> to help understand its data layer. Definitely worth a read.</p><h4>Components abstraction</h4><p>Once most of my stateless and stateful pages were created, I took a bit of time to refactor some parts of the code. I created a src/components/ folder and added a few components to help improve maintainability of the project.</p><p><a href=\"https://github.com/peduarte/dona-rita-website/tree/master/src/components\">Here’s a list</a> of the components I abstracted.</p><h3>Moving from GH Pages to Netlify</h3><p>I was happy with Github Pages, but <a href=\"https://netlify.com\">Netlify</a> is so good, so fast, so easy to use that it’s hard to say no… it offers instant rollbacks, one-click SSL, prerendering, deploy previews, and many more awesome features.</p><p>When a new Pull Request is created, Netlify automatically generates a <a href=\"https://www.netlify.com/blog/2016/07/20/introducing-deploy-previews-in-netlify/\">Deploy Preview</a>. This feature is pure gold. It allows you and other members of the team to see how the changes affect the project.</p><p>When the PR gets merged to the master branch, then Netlify triggers the production deployment. I’ve turned on <a href=\"https://www.netlify.com/blog/2016/07/18/shiny-slack-notifications-from-netlify/\">Slack Notification</a>, so everyone in the team knows when a new production deployment has happened 🎉</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JoxGvcyy6ShE1CWOlx1Clw.png\" /><figcaption>Dona Rita Slack showing Netlify’s notifications</figcaption></figure><h3>Adding Netlify CMS</h3><p><a href=\"https://www.netlifycms.org/\">Netlify CMS</a> is an open source content management system for Git workflows. I decided to give it a try as it seemed surprisingly simple.</p><p>The way I set it up was by authenticating Netlify CMS with Github via <a href=\"https://www.netlifycms.org/docs/authentication-backends/\">Netlify Identity</a>. As with anything Netlify related, it was only a couple of clicks.</p><p>I used Netlify CMS to manage my “Shops” data. The way I am managing this data is by having one Markdown file per Shop. You can see the source for the Shops page <a href=\"https://github.com/peduarte/dona-rita-website/blob/master/src/pages/shops/index.js\">here</a>.</p><p>I also created a <a href=\"https://www.netlifycms.org/docs/configuration-options/\">configuration</a> file. Through this file I was able to create collections and to define which fields can be modified via the CMS.</p><p>You can see the source for Netlify config <a href=\"https://github.com/peduarte/dona-rita-website/blob/master/static/admin/config.yml\">here</a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rvx1jcZ0vDQaXN2xeYy2oA.png\" /><figcaption>Netlify CMS configuration</figcaption></figure><p>If you’re interested in adding a CMS to your static site, I’d definitely recommend using Netlify CMS. You can read <a href=\"https://www.netlifycms.org/docs\">the docs here</a>.</p><h3>Wrapping up</h3><p>I’m really happy with these changes. Gatsby, Netlify and Netlify CMS are all game changers. And when used together, they work in complete harmony.</p><p>If you’re as interested in JAMstack as I do, I’d highly recommend using these tools.</p><p>If you have any advice on how anything here can be improved, please feel free to let me know 🙌</p><p>The initial Jekyll-to-Gatsby migration took me about 6 hours (including having to learn Gatsby and GraphQL).</p><h3>Source Code</h3><p>The code for Dona Rita’s website is <a href=\"https://github.com/peduarte/dona-rita-website/\">public on Github</a>. Feel free to browse around!</p><h3>More helpful resources and links</h3><ul><li><a href=\"https://julesforrest.com/moving-to-gatsby/\">Moving to Gatsby</a> <em>by </em><a href=\"https://twitter.com/julesforrest\"><em>Jules Forrest</em></a></li><li><a href=\"https://www.gatsbyjs.org/blog/2017-11-08-migrate-from-jekyll-to-gatsby/\">Migrate from Jekyll to Gatsby</a> <em>by </em><a href=\"https://twitter.com/@singuerinc\"><em>Nahuel Scotti</em></a></li><li><a href=\"https://jamstack.org/\">JAMstack website</a></li><li><a href=\"https://www.netlify.com/blog/2017/03/16/smashing-magazine-just-got-10x-faster/\">Smashing Magazine just got 10x faster</a></li><li><a href=\"https://medium.com/@kyle.robert.gill/ridiculously-easy-image-optimization-with-gatsby-js-59d48e15db6e\">Image Optimization Made Easy with Gatsby.js</a> <em>by </em><a href=\"https://medium.com/@kyle.robert.gill?source=post_header_lockup\"><em>Kyle Gill</em></a></li><li>Page loading bar with <a href=\"https://www.gatsbyjs.org/packages/gatsby-plugin-nprogress/\\\"><em>gatsy-plugin-nprogress</em></a></li><li>Support for SASS/SCSS with <a href=\"https://www.gatsbyjs.org/packages/gatsby-plugin-sass/\"><em>gatsby-plugin-sass</em></a></li><li>Default Netlify CMS implementation with <a href=\"https://www.gatsbyjs.org/packages/gatsby-plugin-netlify-cms/\"><em>gatsby-plugin-netlify-cms</em></a></li><li>React Google Analytics module with <a href=\"https://github.com/react-ga/react-ga\"><em>react-ga</em></a></li><li>Code images by <a href=\"https://carbon.now.sh\">Carbon</a></li></ul><p>Thanks for reading. Find me on <a href=\"https://twitter.com/peduarte\">Twitter</a> 🐤</p><p>👋</p><h3>📣 Update</h3><p>Since writing this Gatsby v2 has been released, including some awesome improvements 🎉</p><p>Please refer to the official <a href=\"https://www.gatsbyjs.org/docs/migrating-from-v1-to-v2/\">migration guide here</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a300735e2c5d\" width=\"1\" height=\"1\"><hr><p><a href=\"https://medium.com/netlify/jamstack-with-gatsby-netlify-and-netlify-cms-a300735e2c5d\">JAMstack with Gatsby, Netlify and Netlify CMS</a> was originally published in <a href=\"https://medium.com/netlify\">Netlify</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"],"path":"jamstack-with-gatsby-netlify-and-netlify-cms-a300735e2c5d"}]}